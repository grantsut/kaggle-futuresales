{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data loading and preprocessing, utility function definition"},{"metadata":{},"cell_type":"markdown","source":"## Imports and data loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport itertools\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.core.debugger import set_trace\nfrom pandas.tseries.offsets import MonthEnd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgbm\n# A few functions are imported from a utility script, have a look at it if you like.\nimport futuresalesutility as fu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the data and convert the date column in the training data to the datetime dtype to enable datetime operations."},{"metadata":{"trusted":true},"cell_type":"code","source":"item_categories_extra = pd.read_csv('../input/predict-future-sales-extra/item_categories_enhanced.csv')\nitem_categories_extra = item_categories_extra.drop(columns=['category_name', 'supercategory', 'platform'])\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\ntrain = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\ntest = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\n\ntrain[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"* Correct duplicate shop names\n* Drop a few duplicate items in the training set\n* Drop shops which are not in the test set (these are only a few of these and they tend to be strange in some way, e.g. low sales, not operating for long)+\n* Drop categories 8 and 80 as these are for tickets to an annual exhibition which are not sold in the test month\n* Remove outliers and negative values for the item_cnt_day and item_price features (these are few in number but cause problems when generating some features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correct shop labels\ntrain.loc[train.shop_id == 0, \"shop_id\"] = 57\ntrain.loc[train.shop_id == 1, \"shop_id\"] = 58\ntrain.loc[train.shop_id == 11, \"shop_id\"] = 10\n# Drop shops not in test set\ntestshops = test.shop_id.unique()\ntrain = train.loc[train[\"shop_id\"].isin(testshops), :]\ndel testshops\n# Drop duplicates\ntrain = train.drop_duplicates()\n# Drop categories 8 and 80\ntrain = train.merge(items[[\"item_id\", \"item_category_id\"]], on=\"item_id\", how=\"left\")\ntrain = train[~train.item_category_id.isin([8, 80])]\n# Clip outliers and  remove items with a negative sales price or item_cnt_day\ntrain = train.query(\"(item_price>0) & (item_cnt_day>0)\")\ntrain.loc[:, \"item_price\"] = train.loc[:, \"item_price\"].clip(0, train[\"item_price\"].quantile(0.9999))\ntrain.loc[:, \"item_cnt_day\"] = train.loc[:, \"item_cnt_day\"].clip(0, train[\"item_cnt_day\"].quantile(0.999))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Create a training matrix similar to the test items by aggregating the sales to the month level and creating items for every possible combination of shops and items featured in each individual month of the training data. Additionally, concatenate test to train data to enable creation of features for the test items."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_testlike_train(sales_train, test=None):\n    # Create a date_block_num / item_id / shop_id index using all combinations of item_id and shop_id occurring within each date_block\n    # Optionally concatenate the test items to the end\n    indexlist = []\n    for i in sales_train.date_block_num.unique():\n        x = itertools.product(\n            [i],\n            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n        )\n        indexlist.append(np.array(list(x)))\n    df = pd.DataFrame(\n        data=np.concatenate(indexlist, axis=0), columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    # Add revenue column to sales_train\n    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n    )\n\n    # Merge the grouped data with the index\n    df = df.merge(sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],)\n\n    if test is not None:\n        test[\"date_block_num\"] = 34\n        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n        test[\"item_id\"] = test.item_id.astype(np.int16)\n        test = test.drop(columns=\"ID\")\n\n        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n\n    # Fill empty item_cnt entries with 0\n    df.item_cnt_month = df.item_cnt_month.fillna(0)\n    df.item_revenue_month = df.item_revenue_month.fillna(0)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = create_testlike_train(train, test)\nmatrix = fu.reduce_mem_usage(matrix, silent=False)\noldcols = matrix.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering  \nIn this section predictor feature columns are generated and added to the matrix"},{"metadata":{},"cell_type":"markdown","source":"## Time features\nTime features such as item and shop age. Includes some features with day resolutions, such as the number of days since the last sale of the same item, and average sales per day in the previous month, taking into account items or shops which were not available for the whole month (items are assumed to be available from the the date of their first sale and are assumed to be released in all shops on the same date).  \n\nOther features are the date of the first item sale anywhere and in the same shop, the time since the first and last sales anywhere and in the same shop, and the length of the current month. The datetime dtype is used for calculations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_time_features(m, train, correct_item_cnt_day=False):\n    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n\n    def item_shop_age_months(m):\n        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(\n            lambda x: x - x.min()\n        )\n        m[\"new_item\"] = m[\"item_age\"] == 0\n        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n        m[\"shop_age\"] = (\n            m.groupby(\"shop_id\")[\"date_block_num\"]\n            .transform(lambda x: x - x.min())\n            .astype(\"int8\")\n        )\n        m[\"new_shop\"] = m.shop_age == 0\n        m[\"new_shop\"] = m[\"new_shop\"].astype(\"int8\")\n        return m\n\n    m = item_shop_age_months(m)\n\n    # Add dummy values for the test month so that features are created correctly\n    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n    dummies = dummies.assign(\n        date=pd.to_datetime(\"2015-11-30\"),\n        item_price=1,\n        item_cnt_day=0,\n        item_revenue_day=0,\n    )\n    train = pd.concat([train, dummies])\n    del dummies\n\n    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n    month_last_day[~month_last_day.dt.is_month_end] = (\n        month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n    )\n    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n    month_first_day[~month_first_day.dt.is_month_start] = (\n        month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n    )\n    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n    first_shop_item_date = (\n        train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n    )\n\n    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n    m = m.merge(\n        first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\"\n    )\n\n    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n    m[\"item_in_shop_days\"] = (\n        m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n    )\n    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] / m[\"item_in_shop_days\"]\n    m[\"month_length\"] = m[\"month_length\"].dt.days\n\n    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n    m[\"item_first_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n    m[\"shop_item_first_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n    m[\"item_first_sale_days\"] = m[\"item_first_sale_days\"].dt.days.fillna(0).clip(lower=0)\n    m[\"shop_item_first_sale_days\"] = (\n        m[\"shop_item_first_sale_days\"].dt.days.fillna(0).clip(lower=0)\n    )\n\n    # Change the first sale date feature to integer format indexed from the first day in the training data\n    m[\"first_day\"] = train.date.min()\n    m[\"first_item_date\"] = (m[\"first_item_date\"] - m[\"first_day\"]).dt.days\n    m[\"first_shop_item_date\"] = (m[\"first_shop_item_date\"] - m[\"first_day\"]).dt.days\n\n    # Add days since last sale\n    def last_sale_days(matrix):\n        last_shop_item_dates = []\n        last_item_dates = []\n        for dbn in tqdm(range(1, 35)):\n            lsid_temp = (\n                train.query(f\"date_block_num<{dbn}\")\n                .groupby([\"shop_id\", \"item_id\"])\n                .date.max()\n                .rename(\"last_shop_item_sale_date\")\n                .reset_index()\n            )\n            lid_temp = (\n                lsid_temp.groupby(\"item_id\")\n                .last_shop_item_sale_date.max()\n                .rename(\"last_item_sale_date\")\n                .reset_index()\n            )\n            lsid_temp[\"date_block_num\"] = dbn\n            lid_temp[\"date_block_num\"] = dbn\n            last_shop_item_dates.append(lsid_temp)\n            last_item_dates.append(lid_temp)\n        last_shop_item_dates = pd.concat(last_shop_item_dates)\n        last_item_dates = pd.concat(last_item_dates)\n        matrix = matrix.merge(\n            last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\"\n        )\n        matrix = matrix.merge(\n            last_item_dates, on=[\"date_block_num\", \"item_id\"], how=\"left\"\n        )\n\n        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n            m.loc[m[feat_name] > 2000, feat_name] = missingval\n            m.loc[m[feat_name].isna(), feat_name] = missingval\n            return m\n\n        matrix = days_since_last_feat(\n            matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999\n        )\n        matrix = days_since_last_feat(\n            matrix, \"last_item_sale_days\", \"last_item_sale_date\", 9999\n        )\n        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\", \"last_item_sale_date\"])\n        return matrix\n\n    m = last_sale_days(m)\n    # Month\n    m[\"month\"] = m[\"month_first_day\"].dt.month\n\n    m = m.drop(\n        columns=[\n            \"first_day\",\n            \"month_first_day\",\n            \"month_last_day\",\n            \"first_shop_date\",\n            \"item_in_shop_days\",\n            \"first_shop_item_date\",\n        ]\n    )\n\n    if correct_item_cnt_day == True:\n        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n\n    return m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = add_time_features(matrix, train, False)\nprint(\"Time features created\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additionally clip the corrected monthly item counts to remove the effect of outliers, but save the unclipped version also."},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix['item_cnt_month_unclipped'] = matrix.loc[:,'item_cnt_month']\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(lower=0, upper=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Price features  \n\nLast mean price for a month in which the item was sold, the difference of the last price from the historical mean (calculated with an expanding window), the difference of the last mean price from the first sale price in the data (proxy for release price), and the difference of the last mean price from the mean for the category in the same month.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_price_features(matrix, train):\n    # Get mean prices per month from train dataframe\n    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n    price_features = pd.DataFrame(price_features)\n    price_features = price_features.reset_index()\n    # Calculate expanding mean and join to dataframe\n    emp = price_features.groupby(\"item_id\").item_price.expanding().mean()\n    emp = emp.reset_index(level=0)\n    price_features[\"exp_mean_price\"] = emp[\"item_price\"]\n    del emp\n    # Get first sale price for item\n    price_features[\"start_price\"] = price_features.groupby(\n        \"item_id\"\n    ).item_price.transform(lambda x: x.to_numpy()[0])\n    # Calculate  differences from start and mean prices\n    price_features[\"norm_diff_exp_mean_price\"] = (\n        price_features[\"item_price\"] - price_features[\"exp_mean_price\"]\n    ) / price_features[\"exp_mean_price\"]\n    price_features[\"norm_diff_start_price\"] = (\n        price_features[\"item_price\"] - price_features[\"start_price\"]\n    ) / price_features[\"start_price\"]\n    # Calculate normalized differences from start and mean prices\n    price_features[\"norm_diff_exp_mean_price\"] = (\n        price_features[\"item_price\"] - price_features[\"exp_mean_price\"]\n    ) / price_features[\"exp_mean_price\"]\n    # Calculate normalized differenced from mean category price per month\n    price_features = price_features.merge(\n        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n    )\n    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n        [\"date_block_num\", \"item_category_id\"]\n    )[\"item_price\"].transform(lambda x: (x - x.mean()) / x.mean())\n    # Retain only the necessary features\n    price_features = price_features[\n        [\n            \"date_block_num\",\n            \"item_id\",\n            \"item_price\",\n            \"norm_diff_exp_mean_price\",\n            \"norm_diff_start_price\",\n            \"norm_diff_cat_price\",\n        ]\n    ]\n\n    features = [\n        \"item_price\",\n        \"norm_diff_exp_mean_price\",\n        \"norm_diff_start_price\",\n        \"norm_diff_cat_price\",\n    ]\n    newnames = [\"last_\" + f for f in features]\n    aggs = {f: \"last\" for f in features}\n    renames = {f: \"last_\" + f for f in features}\n\n    features = []\n    for dbn in tqdm(range(1, 35)):\n        f_temp = (\n            price_features.query(f\"date_block_num<{dbn}\")\n            .groupby(\"item_id\")\n            .agg(aggs)\n            .rename(columns=renames)\n        )\n        f_temp[\"date_block_num\"] = dbn\n        features.append(f_temp)\n    features = pd.concat(features).reset_index()\n\n    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n    matrix[newnames] = matrix[newnames].fillna(999999)\n    return matrix\n\n\nmatrix = add_price_features(matrix, train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features"},{"metadata":{},"cell_type":"markdown","source":"Add item categories and supercategories, e.g. \"video games\", \"music\", \"PS4\" (uses custom item categories dataframe with manually-chosen categories)"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = matrix.merge(items.drop(columns='item_name'), on='item_id', how='left')\nmatrix = matrix.merge(item_categories_extra, on='item_category_id', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"City that the shop is located in, extracted from the shop name\n(from https://www.kaggle.com/dlarionov/feature-engineering-xgboost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_city_codes(matrix, shops):\n    shops.loc[\n        shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n    ] = 'СергиевПосад ТЦ \"7Я\"'\n    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n    shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n    shop_labels = shops[[\"shop_id\", \"city_code\"]]\n    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n    return matrix\n\nmatrix = add_city_codes(matrix, shops)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n# matrix.to_pickle(\"matrixcheckpoint_1.pk1\")\nprint(\"Saved matrixcheckpoint 1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simple lags features, i.e. the value of a feature for the same item-shop combination from n months back"},{"metadata":{},"cell_type":"markdown","source":"### Rolling mean features\nSales from previous months are a good predictor of future sales, but chance fluctuations mean that an average of several previous months may be more reliable than counts from a single month. Pandas has several windowing functions for time series built in, 3 of which are demonstrated below : expanding (all previous timepoints), rolling (a fixed number of previous timepoints) and exponential (a weighted window with weights which decrease with time distance before the current point). "},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_id = 16\nitem_id = 482\nim = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\nim['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\nim['expanding'] = im['item_cnt_month'].expanding().mean()\nim['rolling_12'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\nim = im.set_index('date_block_num')\nax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A weakness of the pandas windowed functions is that they do not insert zeros for missing timepoints, which is a problem when calculating values for low-volume items which do not record a sale in some months of their availability. This is solved here by creating a sales matrix which contains entries for all items in all months, and using this to generate windowed features. Entries for months prior to an items availability are set to NaN so that they are not included in calculations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_full_train_test(sales_train, test, clip_target=20.0):\n    # Create a train set with all items and all shops in all date blocks.\n    # Add revenue column to sales_train\n    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n    )\n    # Create sets of items, shops and date blocks\n    item_ids = set(sales_train.item_id).union(set(test.item_id))\n    shop_ids = set(sales_train.shop_id).union(set(test.shop_id))\n    date_block_nums = set(sales_train.date_block_num).union(set([34]))\n    # Create all permutations as indexes\n    indexdataframe = pd.DataFrame(\n        np.array(list(itertools.product(date_block_nums, item_ids, shop_ids))),\n        columns=[\"date_block_num\", \"item_id\", \"shop_id\"],\n    )\n    m = indexdataframe.merge(sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"item_id\", \"shop_id\"])\n    m.item_cnt_month = m.item_cnt_month.fillna(0)\n    m.item_revenue_month = m.item_revenue_month.fillna(0)\n    shop_first_sales = train.groupby(\"shop_id\").date_block_num.min().rename(\"shop_first_month\")\n    shop_last_sales = train.groupby(\"shop_id\").date_block_num.max().rename(\"shop_last_month\")\n    item_first_sales = train.groupby(\"item_id\").date_block_num.min().rename(\"item_first_month\")\n    m = m.merge(shop_first_sales, on=\"shop_id\", how=\"left\")\n    m = m.merge(shop_last_sales, on=\"shop_id\", how=\"left\")\n    m = m.merge(item_first_sales, on=\"item_id\", how=\"left\")\n    mask = (\n        (m.date_block_num < m.shop_first_month)\n        | (m.date_block_num < m.item_first_month)\n        | (m.date_block_num > m.shop_last_month)\n    )\n    m = m.drop(columns=[\"shop_first_month\", \"shop_last_month\", \"item_first_month\"])\n    m.loc[mask, [\"item_cnt_month\", \"item_revenue_month\"]] = np.nan\n    \n    if clip_target is not None:\n        m['item_cnt_month'] = m['item_cnt_month'].clip(upper=clip_target)\n    m = fu.reduce_mem_usage(m)\n    return m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = create_full_train_test(train, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define general purpose rolling mean function"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\ndef add_rolling_ME(\n    matrix,\n    features,\n    window=3,\n    min_periods=1,\n    ewm=False,\n    source_matrix=None,\n    target_feature=\"item_cnt_month\",\n    aggregation=\"mean\",\n    rolling_aggregation=\"mean\",\n    expanding=False,\n):\n    \"\"\"Add a rolling mean item_cnt_month feature for a specificed categorical feature\n    or features. Calculates using the feature matrix and therefore does not insert zeros\n    for features with zero sales in a specific month (use feature specific rolling ME\n    functions for rolling item or item-shop features)\"\"\"\n    features = fu.list_if_not(features)\n    if source_matrix is None:\n        source_matrix = matrix\n    im = source_matrix.groupby(features + [\"date_block_num\"])[target_feature].agg(aggregation)\n    im = im.reset_index(level=features + [\"date_block_num\"])\n    im = fu.reduce_mem_usage(im)\n    if ewm:\n        feat_name = f\"{'_'.join(features)}_{target_feature}_{aggregation}_ewm_hl_{window}\"\n        print(f'Creating feature \"{feat_name}\"')\n        im[feat_name] = im.groupby(features)[target_feature].transform(lambda x: x.ewm(halflife=window, min_periods=min_periods).mean())\n    elif expanding:\n        feat_name = f\"{'_'.join(features)}_{target_feature}_{aggregation}_expanding_{rolling_aggregation}\"\n        print(f'Creating feature \"{feat_name}\"')\n        im[feat_name] = im.groupby(features)[target_feature].transform(lambda x: x.expanding(min_periods=min_periods).aggregate(rolling_aggregation))\n    else:\n        feat_name = (\n            f\"{'_'.join(features)}_{target_feature}_{aggregation}_rolling_{rolling_aggregation}_win_{window}\"\n        )\n        print(f'Creating feature \"{feat_name}\"')\n        im[feat_name] = im.groupby(features)[target_feature].transform(lambda x: x.rolling(window=window, min_periods=min_periods).aggregate(rolling_aggregation))\n    im = im.drop(columns=target_feature)\n    im.loc[:, 'date_block_num'] += 1\n    matrix = matrix.merge(im, on=['date_block_num'] + features, how='left')\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implement rolling mean / median / minimum / maximum features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shop-item specific windowed features\nmatrix = add_rolling_ME(matrix, features=[\"shop_id\", \"item_id\"], window=12, source_matrix=m)\ngc.collect()\nmatrix = add_rolling_ME(matrix, features=[\"shop_id\", \"item_id\"], window=1, ewm=True, source_matrix=m)\ngc.collect()\nmatrix = add_rolling_ME(matrix, features=[\"shop_id\", \"item_id\"], window=1, ewm=True, source_matrix=m, aggregation=\"median\")\ngc.collect()\n# Item all shops features, various stats\nmatrix = add_rolling_ME(matrix, features=[\"item_id\"], window=12, source_matrix=m)\ngc.collect()\nmatrix = add_rolling_ME(matrix, features=[\"item_id\"], window=1, ewm=True, source_matrix=m)\ngc.collect()\nmatrix = add_rolling_ME(\n    matrix, [\"item_id\"], window=12, target_feature=\"item_revenue_month\", aggregation=\"sum\", \n)\ngc.collect()\nmatrix = add_rolling_ME(matrix, features=[\"item_id\"], window=12, source_matrix=m, aggregation=\"median\")\ngc.collect()\nmatrix = add_rolling_ME(\n    matrix, features=[\"item_id\"], window=1, ewm=True, source_matrix=m, aggregation=\"median\"\n)\ngc.collect()\nmatrix = add_rolling_ME(matrix, features=[\"item_id\"], window=12, source_matrix=m, aggregation=\"max\")\ngc.collect()\nmatrix = add_rolling_ME(matrix, features=[\"item_id\"], window=12, source_matrix=m, aggregation=\"var\")\ngc.collect()\ndel(m)\nprint(\"Rolling item features created\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The item age max 12 feature is used for mean encoding features later\nmatrix[\"item_age_max_12\"] = matrix[\"item_age\"].clip(upper=12)\n\n# Item category rolling feature (necessary for ratio feature later)\nmatrix = add_rolling_ME(matrix, [\"item_category_id\"], window=12)\n\n# Shop - item category interaction features\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=12)\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=1, ewm=True)\nmatrix = add_rolling_ME(\n    matrix,\n    [\"shop_id\", \"item_category_id\"],\n    window=12,\n    target_feature=\"item_revenue_month\",\n    aggregation=\"sum\",\n)\ngc.collect()\n# Shop features\nmatrix = add_rolling_ME(matrix, [\"shop_id\"], window=12)\nmatrix = add_rolling_ME(\n    matrix, [\"shop_id\"], window=12, target_feature=\"item_revenue_month\", aggregation=\"sum\"\n)\n# Shop digital features (almost equivalent to the shop features)\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"digital\"], window=12)\ngc.collect()\n# Item age - item category \nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"item_category_id\"], window=1, ewm=True)\nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"item_category_id\"], window=12)\nmatrix = add_rolling_ME(\n    matrix,\n    [\"item_age_max_12\", \"item_category_id\"],\n    window=12,\n    target_feature=\"item_revenue_month\",\n    aggregation=\"sum\",\n)\ngc.collect()\n# Item age - shop - category interaction rolling mean\nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"shop_id\", \"item_category_id\"], window=1, ewm=True)\nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"shop_id\", \"item_category_id\"], window=12)\nmatrix = add_rolling_ME(\n    matrix,\n    [\"item_age_max_12\", \"shop_id\", \"item_category_id\"],\n    window=12,\n    target_feature=\"item_revenue_month\",\n    aggregation=\"sum\",\n)\n\n# Item price rolling mean\nmatrix = add_rolling_ME(matrix, [\"item_id\"], window=1, target_feature=\"last_item_price\", ewm=True)\n\n# Shop - item category interaction features\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=1, ewm=True, aggregation=\"median\")\n\n# New item - item category mean encode\nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"item_category_id\"], window=12, aggregation=\"median\")\n\n# Shop - item category interaction features\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=0.5, ewm=True, aggregation=\"min\")\n\n# Shop - item category interaction features\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=12, aggregation=\"max\")\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=0.5, ewm=True, aggregation=\"max\")\n\n# New item - item category mean encode\nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"item_category_id\"], window=1, ewm=True, aggregation=\"max\")\nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"item_category_id\"], window=6, aggregation=\"max\")\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rolling variance features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# New item - item category mean\nmatrix = add_rolling_ME(\n    matrix,\n    [\"item_age_max_12\", \"item_category_id\"],\n    window=12,\n    rolling_aggregation=\"var\"\n)\n\n# New item - shop - category interaction rolling mean\nmatrix = add_rolling_ME(matrix, [\"item_age_max_12\", \"shop_id\", \"item_category_id\"], window=12, rolling_aggregation=\"var\")\nmatrix = add_rolling_ME(\n    matrix,\n    [\"item_age_max_12\", \"shop_id\", \"item_category_id\"],\n    window=12,\n    target_feature=\"item_revenue_month\",\n    aggregation=\"sum\",\n    rolling_aggregation=\"var\"\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expanding sum features (also functions as a marker of a previous item sale)"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = add_rolling_ME(\n    matrix,\n    [\"shop_id\", \"item_id\"],\n    target_feature=\"item_cnt_month\",\n    aggregation=\"sum\",\n    rolling_aggregation=\"sum\",\n    expanding=True,\n)\nmatrix = add_rolling_ME(\n    matrix,\n    [\"item_id\"],\n    target_feature=\"item_revenue_month\",\n    aggregation=\"sum\",\n    rolling_aggregation=\"sum\",\n    expanding=True,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More expanding mean features"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = add_rolling_ME(\n    matrix,\n    [\"month\", \"item_category_id\"],\n    expanding=True,\n)\nmatrix = add_rolling_ME(\n    matrix,\n    [\"month\", \"supercategory_id\"],\n    expanding=True,\n)\nmatrix = add_rolling_ME(\n    matrix,\n    [\"month\", \"shop_id\"],\n    expanding=True,\n)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean sales of same category features with the same age in past data, using expanding mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = add_rolling_ME(matrix, [\"shop_id\", \"supercategory_id\", \"item_age_max_12\"], expanding=True)\nmatrix = add_rolling_ME(matrix, [\"supercategory_id\", \"item_age_max_12\"], expanding=True)\nmatrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\", \"item_age_max_12\"], expanding=True)\nmatrix = add_rolling_ME(matrix, [\"item_category_id\", \"item_age_max_12\"], expanding=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n# matrix.to_pickle(\"matrixcheckpoint_2.pk1\")\nprint(\"Saved matrixcheckpoint 2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_lag_feature(matrix, lag_feature, lags):\n    targetseries = matrix.loc[:,['date_block_num', 'item_id', 'shop_id'] + [lag_feature]]\n    targetseries = targetseries.set_index(['date_block_num', 'item_id', 'shop_id'])\n    targetseries = targetseries[lag_feature]\n    for lag in tqdm(lags):\n        matrix = fu.add_lag_feature(matrix, targetseries, ['item_id', 'shop_id'], lag, fillna=0)\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1,2,3])\nmatrix = simple_lag_feature(matrix, 'item_cnt_month_unclipped', lags=[1])\nmatrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1, 2])\nmatrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\ngc.collect()\nprint(\"Lag features created\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean encoding, i.e. the mean value of a target feature for each level of a categorical feature or combination of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_apply_ME(\n    matrix, grouping_fields, name, lags=[1], source_frame=None, target=\"item_cnt_month\", aggregation=\"mean\",\n):\n    if source_frame is None:\n        source_frame = matrix\n    grouping_fields = fu.list_if_not(grouping_fields)\n    me_series = (\n        source_frame.groupby([\"date_block_num\"] + grouping_fields)[target].agg(aggregation).rename(name)\n    )\n    matrix = fu.apply_lags(matrix, me_series, grouping_fields, lags)\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = create_apply_ME(matrix, [\"item_id\"], \"item_id_ME\", lags=[1])\nmatrix = create_apply_ME(matrix, [\"item_id\"], \"item_id_item_cnt_day_avg\", lags=[1,2,3], target=\"item_cnt_day_avg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = create_apply_ME(matrix, [\"item_id\"], \"item_id_ME_unclipped\", target=\"item_cnt_month_unclipped\")\nmatrix = create_apply_ME(matrix, [\"item_category_id\"], \"item_category_id_ME\", [1, 2, 12])\nmatrix = create_apply_ME(matrix, [\"supercategory_id\"], \"supercategory_id_ME\", [1, 12])\nmatrix = create_apply_ME(matrix, [\"platform_id\"], \"platform_id_ME\", [1, 12])\nmatrix = create_apply_ME(matrix, \n    [\"shop_id\", \"item_category_id\"], \"shop_id_item_category_id_ME\", [1]\n)\nmatrix = create_apply_ME(matrix, [\"shop_id\", \"digital\"], \"shop_id_digital_ME\", [1])\nmatrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"], \"city_code_item_id_ME\", [1], target=\"item_cnt_day_avg\")\nmatrix[\"item_age_max_12\"] = matrix[\"item_age\"].clip(upper=12)\nmatrix = create_apply_ME(matrix, [\"item_age_max_12\", \"item_category_id\"], \"item_age_item_category_id_ME\", [1], target='item_cnt_month')\nmatrix = create_apply_ME(matrix, [\"item_age_max_12\", \"supercategory_id\"], \"new_item_supercategory_id_ME\", [1])\nmatrix = create_apply_ME(matrix, [\"item_age_max_12\", \"shop_id\", \"item_category_id\"], \"new_item_shop_id_item_category_id_ME\", [1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ratios between recent sales and rolling 12 month means"},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n    matrix[\"item_id_ME_lag_1\"] / matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n).fillna(1)\nmatrix[\"item_category_id_item_cnt_lag_1_12_ratio\"] = (\n    matrix[\"item_category_id_ME_lag_1\"] / matrix[\"item_category_id_item_cnt_month_mean_rolling_mean_win_12\"]\n).fillna(1)\nmatrix[\"shop_id_item_category_id_item_cnt_lag_1_12_ratio\"] = (\n    matrix[\"shop_id_item_category_id_ME_lag_1\"]\n    / matrix[\"shop_id_item_category_id_item_cnt_month_mean_rolling_mean_win_12\"]\n).fillna(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\ngc.collect()\nprint(\"Mean encoding features created\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percentage change in an aggregate feature over a specified period, e.g. percentage change in total shop revenue compared to the previous month"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_pct_change(\n    df, group_feats, quantity=\"item_cnt_month\", agg_function=\"mean\", periods=1, lag=1, clip_value=None,\n):\n    \"\"\"\n    Adds a column of month-to-month proportion change values for an aggregate of a feature\n    \n    Arguments:\n    df (Dataframe): dataframe to add feature to\n    group_feats (str or list): column or list of columns to group items by when generating\n    aggregate features\n    quantity (str): column to aggregate\n    agg_function (str): aggregate function to use, passed to pandas GroupBy.agg() method\n    periods (int or str): time deltas to use when calculating percentage changes of the\n    aggregate feature\n    lag (int): lag value for joining the new feature to the dataframe\n    clip_value: positive absolute value used to set the negative and positive maximum\n    values of the new feature (delta values can be very large due to zero values)\n    \"\"\"\n    # Put string arguments for arguments \"periods\" and \"group_feats\" into lists\n    periods = fu.list_if_not(periods, int)\n    group_feats = fu.list_if_not(group_feats)\n    # group_feats_full: list of grouping columns + time column\n    group_feats_full = [\"date_block_num\"] + group_feats\n    # Create a template index of all group feature values in all months (original df misses levels in months where they have no sales)\n    idx = pd.MultiIndex.from_product([df[col].unique() for col in group_feats_full], names=group_feats_full)\n    template = pd.DataFrame(index=idx)\n    template = template.sort_index()\n    # Create aggregate feature and merge the results with the template\n    aggs = df.groupby(group_feats_full)[quantity].agg(agg_function).fillna(value=0)\n    template = template.merge(aggs, on=group_feats_full, how=\"left\")\n    # Generate pct_change feature for each specified period, clip values then add to df with a lag of 1\n    for period in periods:\n        feat_name = \"_\".join(group_feats + [quantity] + [agg_function] + [\"delta\"] + [str(period)])\n        print(f\"Adding feature {feat_name}\")\n        feature_series = (\n            template.groupby(group_feats)[quantity]\n            .transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\"))\n            .fillna(value=0)\n            .rename(feat_name)\n        )\n        if clip_value is not None:\n            feature_series = feature_series.clip(lower=-clip_value, upper=clip_value)\n        # Add to df with a specified lag\n        df = fu.add_lag_feature(df, feature_series, group_feats, lag=lag)\n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean item price change\nmatrix = add_pct_change(matrix, [\"item_id\"], \"last_item_price\", clip_value=3, lag=0)\n# Mean item sales change\nmatrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\n# Mean item category sales change\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\n# Mean sales change across everything\nmatrix = add_pct_change(matrix, [\"shop_id\", \"item_id\"], \"item_cnt_month\", clip_value=3)\n# Mean item revenue change\nmatrix = add_pct_change(matrix, [\"item_id\"], \"item_revenue_month\", agg_function=\"sum\", clip_value=3)\n# Delta 1 features lagged by 12 months, intended to capture seasonal trends\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\nmatrix = add_pct_change(matrix, [\"shop_id\"], \"item_cnt_month\", lag=12, clip_value=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\ngc.collect()\nprint(\"Delta features created\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical interaction features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def interaction_feature(features):\n    feature_name = '_'.join(['interaction'] + features)\n    matrix[feature_name] = matrix[features[0]].apply(str)\n    for feature in features[1:]:\n        matrix[feature_name] = matrix[feature_name] + '_' +  matrix[feature].apply(str)\n    matrix[feature_name] = fu.reduce_mem_usage(matrix[feature_name], allow_categorical=False)\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Month category interactions\ninteraction_feature([\"month\", \"item_category_id\"])\ninteraction_feature([\"month\", \"supercategory_id\"])\ninteraction_feature([\"month\", \"digital\"])\n# New item category interactions\ninteraction_feature([\"new_item\", \"item_category_id\"])\ninteraction_feature([\"new_item\", \"supercategory_id\"])\ninteraction_feature([\"new_item\", \"platform_id\"])\n# Shop - category interactions\ninteraction_feature([\"shop_id\", \"item_category_id\"])\n# Digital shop_id interactions\nmatrix[\"is_shop_55\"] = matrix.shop_id == 55\ninteraction_feature([\"is_shop_55\", \"digital\"])\nmatrix = matrix.drop(columns=[\"is_shop_55\"])\nmatrix = matrix.rename(columns={\"interaction_is_shop_55_digital\":\"interaction_shop_digital\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Artist name feature for music categories  \n\nThe \"item_name\" field of items in the music categories typically begin with the artist's name marked with one of three patterns: either all uppercase, separated from the release title by a doublespace, or separated by dot-space (. )"},{"metadata":{"trusted":true},"cell_type":"code","source":"items.query(\"item_category_id==55\").head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using regex operations, these are extracted and made more homogenous by removal of special characters and converting to uppercase. This method is not perfect and sometimes extracts general terms such as \"Jazz\" instead, but hopefully is useful. Artist names are also count encoded according to their occurences in the items table."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_name_features(\n    matrix,\n    factorize=True,\n    encode_missing=float(\"nan\"),\n    unique_thresh=2,\n    items=items,\n    fillna_value=None,\n    feature_name=\"music_artist\",\n):\n    # This extracts artist names for music categories and adds them as a feature.\n    def extract_artist(st):\n        import re\n\n        st = st.strip()\n        if st.startswith(\"V/A\"):\n            artist = \"V/A\"\n        elif st.startswith(\"СБ\"):\n            artist = \"СБ\"\n        else:\n            # Retrieves artist names using the double space or all uppercase pattern\n            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n            match_dubspace = mus_artist_dubspace.match(st)\n            mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n            match_capsonly = mus_artist_capsonly.match(st)\n            candidates = [match_dubspace, match_capsonly]\n            candidates = [m[0] for m in candidates if m is not None]\n            # Sometimes one of the patterns catches some extra words so choose the shortest one\n            if len(candidates):\n                artist = min(candidates, key=len)\n            else:\n                # If neither of the previous patterns found something, use the dot-space pattern\n                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n                match = mus_artist_dotspace.match(st)\n                if match:\n                    artist = match[0]\n                else:\n                    artist = \"\"\n        artist = artist.upper()\n        artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n        artist = artist.strip()\n        return artist\n\n    music_categories = [55, 56, 57, 58, 59, 60]\n    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[\n        items.item_category_id.isin(music_categories), \"item_name\"\n    ].apply(extract_artist)\n\n    items.loc[items[feature_name] == \"\", feature_name] = float(\"nan\")\n\n    freq_enc_name = feature_name + \"_freq_encode\"\n    # Remove artist names which are associated with less than unique_thresh items\n    n_items = items[feature_name].value_counts().rename(freq_enc_name)\n    items = items.merge(n_items, left_on=feature_name, right_index=True, how=\"left\")\n    items = items.loc[items[freq_enc_name] >= unique_thresh, :]\n\n    if factorize:\n        items[feature_name] = items[feature_name].factorize(na_sentinel=-1)[0]\n        items.loc[items[feature_name] == -1, feature_name] = encode_missing\n\n    matrix = matrix.merge(\n        items.loc[items.item_category_id.isin(music_categories), [\"item_id\", feature_name, freq_enc_name],],\n        on=\"item_id\",\n        how=\"left\",\n    )\n    if fillna_value is not None:\n        matrix[[feature_name, freq_enc_name]] = matrix[[feature_name, freq_enc_name]].fillna(fillna_value)\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = add_name_features(matrix, factorize=True, fillna_value=-1)\n# Music artist rolling mean sale count and revenue *** ADD EXPANDING ***\nmatrix = add_rolling_ME(matrix, [\"music_artist\"], window=12, target_feature=\"item_cnt_month\")\nmatrix = add_rolling_ME(\n    matrix, [\"music_artist\"], window=24, target_feature=\"item_revenue_month\", aggregation=\"sum\"\n)\nprint(\"Created categorical features\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Minor data leak features  \ni.e. counts of new and unique items in the current month  \nThis counts the number of unique and new items sold in the current month and also category, which could have a positive relationship to sales. As this can also be calculated for the test set (assuming that the set of test items is the set of items that were sold in the test month) this is a kind of data leak. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_unique_item_features(matrix):\n    # Adds the number of unique and new items per month and month-category, and these values as a proportion\n    unique_id_features = (\n        matrix.groupby([\"date_block_num\", \"item_category_id\"])\n        .item_id.nunique()\n        .rename(\"unique_items_cat\")\n        .reset_index()\n    )\n    unique_id = (\n        unique_id_features.groupby(\"date_block_num\").unique_items_cat.sum().rename(\"unique_items_month\")\n    )\n    unique_id_features = unique_id_features.merge(\n        unique_id, how=\"left\", left_on=\"date_block_num\", right_index=True,\n    )\n    unique_id_features[\"cat_items_proportion\"] = unique_id_features[\"unique_items_cat\"].div(\n        unique_id_features[\"unique_items_month\"]\n    )\n    new_items_cat = (\n        matrix.loc[matrix.new_item == 1, [\"date_block_num\", \"item_id\", \"item_category_id\"]]\n        .groupby([\"date_block_num\", \"item_category_id\"])\n        .item_id.nunique()\n        .rename(\"new_items_cat\")\n    )\n    unique_id_features = unique_id_features.merge(\n        new_items_cat, left_on=[\"date_block_num\", \"item_category_id\"], right_index=True, how=\"left\",\n    )\n    unique_id_features[\"new_items_cat\"] = unique_id_features[\"new_items_cat\"].fillna(0)\n    new_items_month = (\n        unique_id_features.groupby(\"date_block_num\").new_items_cat.sum().rename(\"new_items_month\")\n    )\n    unique_id_features = unique_id_features.merge(\n        new_items_month, left_on=\"date_block_num\", right_index=True, how=\"left\"\n    )\n    unique_id_features[\"cat_items_new_proportion\"] = (\n        unique_id_features[\"new_items_cat\"] / unique_id_features[\"unique_items_cat\"]\n    )\n    matrix = matrix.merge(unique_id_features, on=[\"date_block_num\", \"item_category_id\"], how=\"left\")\n    return matrix\n\n\nmatrix = add_unique_item_features(matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n# matrix.to_pickle(\"matrixcheckpoint_3.pk1\")\nprint(\"Saved matrixcheckpoint 3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sales of item with item_id one above or below (these tend to be related items)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_neighbor_item_features(matrix, feature, group= ['item_id']):\n    # Item_id minus 1\n    f = matrix.groupby([\"date_block_num\"] + group)[feature].agg(\"mean\")\n    name_minus = \"minus1_\" + feature\n    f.name = name_minus\n    f = f.reset_index([\"date_block_num\"] + group)\n    f[\"item_id\"] = f[\"item_id\"] + 1\n    f['date_block_num'] += 1\n    matrix = matrix.merge(f, on=[\"date_block_num\"] + group, how='left')\n    matrix[name_minus] = matrix[name_minus].fillna(99)\n#     f = f.set_index([\"date_block_num\"] + group)[new_name]\n#     f = f.reset_index([\"date_block_num\"] + group)\n#     matrix = fu.add_lag_feature(matrix, f, group, lag=0)\n#     matrix = matrix.rename(columns={new_name + '_lag_0': new_name})\n    # Item_id plus 1\n    name_plus = \"plus1_\" + feature\n    f = f.rename(columns={name_minus:name_plus})\n    f[\"item_id\"] = f[\"item_id\"] - 2\n    f['date_block_num']\n    matrix = matrix.merge(f, on=[\"date_block_num\"] + group, how='left')\n    matrix[name_plus] = matrix[name_plus].fillna(99)\n#     f = f.set_index([\"date_block_num\"] + group)[new_name]\n#     matrix = fu.add_lag_feature(matrix, f, group, lag=0)\n#     matrix = matrix.rename(columns={new_name + '_lag_0': new_name})\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"item_cnt_day_avg_lag_1\", 'item_id_item_cnt_day_avg_lag_1', 'item_id_item_cnt_month_mean_rolling_mean_win_12']\nfor feature in features:\n    matrix = add_neighbor_item_features(matrix, feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Item name length as a feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef clean_item_name(string):\n    # Removes bracketed terms, special characters and extra whitespace\n    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    string = string.lower()\n    return string\n\nitems[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\nitems['item_name_length'] = items.item_name.apply(len)\nmatrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nmatrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\nmatrix.to_pickle(\"matrixcheckpoint_4.pk1\")\nprint(\"Saved matrixcheckpoint 4\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Similar item name values"},{"metadata":{},"cell_type":"markdown","source":"(mention new items) this uses the fuzzy string matching package FuzzyWuzzy to find items with similar names and return sales count values for these items in their first month of availability, weighted by their similarity to the target name. Bracketed terms are stripped from the item names before matching, because these tend to contain product codes or format information (e.g. DVD, Xbox). "},{"metadata":{"trusted":true},"cell_type":"code","source":"# items.loc[[23, 34, 12],:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def item_name_similarity_frame(itemsdf):\n#     # Makes a dataframe of similarity values between item names calculated with FuzzyWuzzy\n#     # itemsdf must have a supercategory_id field\n#     import re\n#     from itertools import combinations\n\n#     import scipy.sparse as sp\n#     from fuzzywuzzy import fuzz\n\n#     def strip_sq_brackets(string):\n#         return re.sub(r\"\\[.*?\\]\", \"\", string)\n\n#     def strip_rd_brackets(string):\n#         return re.sub(r\"\\(.*?\\)\", \"\", string)\n\n#     items[\"item_name\"] = items[\"item_name\"].apply(strip_sq_brackets).apply(strip_rd_brackets)\n\n#     itnames = items.item_name.to_list()\n#     itsupcats = items.supercategory_id.to_list()\n#     pairs = combinations(items.index, 2)\n#     sims = sp.dok_matrix((len(itnames), len(itnames)), dtype=np.int8)\n\n#     for id1, id2 in tqdm(\n#         pairs, total=len(itnames) * (len(itnames) - 1) / 2, desc=\"Calculating similarity values\"\n#     ):\n#         if itsupcats[id1] != itsupcats[id2]:\n#             pass\n#         else:\n#             sims[id1, id2] = fuzz.token_sort_ratio(itnames[id1], itnames[id2], force_ascii=False)\n\n#     csims = sims.tocsr()\n#     csims = csims + csims.T\n#     csims = pd.DataFrame.sparse.from_spmatrix(csims)\n#     return csims\n\n\n# def make_sim_item_features(matrix, sim_frame, return_fields, sim_thresh=50, max_n=5, max_item_age=12):\n#     storelist = []\n#     mean_fields = [s + \"_all_shops\" for s in return_fields]\n#     for date_block_num in tqdm(range(3, 35), \"Generating similar item name features\"):\n#         for item_age in np.sort(matrix.loc[matrix.date_block_num == date_block_num, \"item_age\"].unique()):\n\n#             def get_sim_item_features(item_ids):\n#                 slist = []\n#                 for item_id in item_ids:\n#                     sim_items = sim_frame.loc[item_id, item_ids_past].nlargest(max_n)\n#                     sim_items = sim_items[sim_items>sim_thresh]\n#                     if len(sim_items)==0:\n#                         pass\n#                     else:\n#                         sim_item_values = (\n#                             past_months.loc[(slice(None), sim_items.index), return_fields]\n#                             .groupby(\"shop_id\")[return_fields]\n#                             .mean()\n#                         )\n#                         sim_item_values[mean_fields] = sim_item_values.mean()\n#                         sim_item_values[\"item_id\"] = item_id\n#                         slist.append(sim_item_values)\n#                 sframe = pd.concat(slist)\n#                 sframe[\"date_block_num\"] = date_block_num\n#                 sframe = sframe.reset_index()\n#                 return sframe\n#             if item_age < date_block_num:\n#                 if item_age >= max_item_age:\n#                     past_months = (\n#                         matrix.query(f\"date_block_num<{date_block_num} & date_block_num>1 & item_age>={item_age}\")\n#                         .groupby([\"shop_id\", \"item_id\"])[return_fields]\n#                         .mean()\n#                     )\n#                     item_ids_past = past_months.index.get_level_values(\"item_id\").unique()\n#                     item_ids = matrix.query(\n#                         f\"date_block_num=={date_block_num} & item_age>={item_age}\"\n#                     ).item_id.unique()\n#                     sframe_new = get_sim_item_features(item_ids)\n#                     storelist = storelist + [sframe_new]\n#                     break\n#                 else:\n#                     past_months = (\n#                         matrix.query(f\"date_block_num<{date_block_num} & date_block_num>1 & item_age=={item_age}\")\n#                         .groupby([\"shop_id\", \"item_id\"])[return_fields]\n#                         .mean()\n#                     )\n#                     item_ids_past = past_months.index.get_level_values(\"item_id\").unique()\n#                     item_ids = matrix.query(\n#                         f\"date_block_num=={date_block_num} & item_age=={item_age}\"\n#                     ).item_id.unique()\n#                     sframe_new = get_sim_item_features(item_ids)\n#                     storelist = storelist + [sframe_new]\n#     sim_item_features = pd.concat(storelist)\n#     sim_item_field_names = {s: \"sim_item_name_\" + s for s in return_fields + mean_fields}\n#     sim_item_features = sim_item_features.rename(columns=sim_item_field_names)\n#     return sim_item_features\n\n\n# def add_sim_item_features(matrix, sim_item_features, fill_na_val=None):\n#     oldcols = matrix.columns\n#     matrix = matrix.merge(sim_item_features, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\")\n#     newcols = matrix.columns.difference(oldcols)\n#     if fill_na_val is not None:\n#         matrix[newcols] = matrix[newcols].fillna(fill_na_val)\n#     return matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add the similar item name sales features. Creating these takes ~10 hours on an i7 laptop, so previously created features can be loaded instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load_data = \"all\"  # \"sim_matrix\" to load the name similarity matrix then compute new features, and \"all\" to load the features.\n# if load_data not in [\"all\", \"sim_matrix\"]:\n#     items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\n#     items = items.merge(\n#         item_categories_extra[[\"item_category_id\", \"supercategory_id\"]], on=\"item_category_id\", how=\"left\"\n#     )\n#     sim_matrix = item_name_similarity_frame(items)\n# elif load_data == \"sim_matrix\":\n#     sim_matrix = pd.read_pickle(\"../input/predict-future-sales-extra/sim_frame_no_brackets.pk1\")\n#     sim_item_features = make_sim_item_features(matrix, sim_matrix, [\"item_cnt_month\", \"item_cnt_day_avg\"], sim_thresh=70, max_n=5)\n#     sim_item_features.to_csv(\"sim_item_features_w_missing.csv.gz\")\n# elif load_data == \"all\":\n#     sim_item_features = pd.read_csv(\"../input/predict-future-sales-extra/sim_item_features_w_missing.csv\")\n\n# matrix = add_sim_item_features(matrix, sim_item_features)\n\n# missingmask = matrix[\"sim_item_name_item_cnt_month\"].isna()\n# matrix.loc[missingmask, \"sim_item_name_item_cnt_month\"] = matrix.loc[\n#     missingmask, \"shop_id_supercategory_id_item_age_max_12_item_cnt_month_mean_expanding_mean\"\n# ]\n# missingmask = matrix[\"sim_item_name_item_cnt_month_all_shops\"].isna()\n# matrix.loc[missingmask, \"sim_item_name_item_cnt_month_all_shops\"] = matrix.loc[\n#     missingmask, \"supercategory_id_item_age_max_12_item_cnt_month_mean_expanding_mean\"\n# ]\n# del(sim_item_features)\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the mean for the supercategory / age combination to fill in missing similar name sales values for cases when no similar names were found, additionally leave the supercategory / age mean as a new feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n# matrix.to_pickle(\"matrixcheckpoint_5.pk1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%reset -f","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model fitting section"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\nimport lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix = pd.read_pickle(\"matrixcheckpoint_4.pk1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generate train, validation, test sets from feature matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_x_y(\n    matrix, test_month, keep_from_month=3, new_target=None\n):\n    if new_target is not None:\n        matrix[\"item_cnt_month\"] = matrix.loc[:, new_target]\n        matrix = matrix.drop(columns=new_target)\n\n    def split_train_test(matrix, test_month=33):\n        # Split the matrix into train and test sets.\n        test = matrix.loc[matrix.date_block_num==test_month, :]\n        train = matrix.loc[matrix.date_block_num < test_month, :]\n        return train, test\n\n    def xysplit(matrix):\n        # Split a train and test set into into x and y sets, with item_cnt as the target y variable\n        y = matrix.item_cnt_month\n        X = matrix.drop(columns=[\"item_cnt_month\"])\n        return (X, y)\n\n    matrix = matrix.drop(\n        columns=[\n            \"item_revenue_month\",\n            \"item_price\",\n            \"item_cnt_month_original\",\n            \"item_cnt_month_unclipped\",\n            \"item_cnt_day_avg\",\n            \"new_item\",\n            \"new_shop\",\n            \"item_age\",\n            \"shop_age\",\n            \"digital\",\n        ],\n        errors=\"ignore\",\n    )\n\n    train, test = split_train_test(matrix, test_month)\n    train = train[train.date_block_num >= keep_from_month]\n    X_train, y_train = xysplit(train)\n    X_test, y_test = xysplit(test)\n    return (X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_categorical_columns(matrix):\n    basic = [\n        \"shop_id\",\n        \"item_category_id\",\n        \"city_code\",\n        \"month\",\n        \"interaction_month_digital\",\n        \"interaction_month_item_category_id\",\n    ]\n    basic = [c for c in basic if c in matrix.columns]\n    matrix.loc[:, basic] = matrix.loc[:, basic].astype(\"category\")\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def censor_lag_features(matrix, candidate_features, replacement_val = 9999):\n    # replace any lag features which are invalid due to the shop / item age being less than the lag\n    def item_lag_feats(lag):\n        return [\n            f\n            for f in candidate_features\n            if f\"lag_{lag}\" in f\n            and \"item\" in f\n            and f[:4] != \"shop\"\n            and \"category\" not in f\n            and \"city\" not in f\n            and \"minus\" not in f\n            and \"plus\" not in f\n            and \"sim\" not in f\n        ]\n\n    def shop_lag_feats(lag):\n        return [\n            f\n            for f in candidate_features\n            if f\"lag_{lag}\" in f\n            and \"shop\" in f\n            and \"category\" not in f\n            and \"city\" not in f\n            and \"minus\" not in f\n            and \"plus\" not in f\n            and \"sim\" not in f\n        ]\n\n    lags = range(1,13)\n    for lag in lags:\n        lag_feats = shop_lag_feats(lag)\n        matrix.loc[matrix.shop_age < lag, lag_feats] = replacement_val\n        lag_feats = item_lag_feats(lag)\n        matrix.loc[matrix.item_age < lag, lag_feats] = replacement_val\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgbm\n\nclass Objective(object):\n    def __init__(self, matrix):\n        matrix = censor_lag_features(matrix, matrix.columns, replacement_val=9999)\n        self.X_train, self.y_train, self.X_valid, self.y_valid = train_test_x_y(\n            matrix, test_month=33, keep_from_month=2,\n            )\n\n    def __call__(self, trial):\n        params = {\n            \"boosting_type\": \"gbdt\",\n            \"device_type\": \"cpu\",\n            \"n_jobs\": 11,\n            \"silent\": True,\n            \"n_estimators\": 1000,\n            \"learning_rate\": 0.1,\n            \"bagging_seed\": 3,\n            \"subsample_for_bin\": 300000,\n            \"max_depth\": -1,\n            \"min_data_in_bin\": 1,\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 255, 2048, log=True),\n            \"cat_smooth\": trial.suggest_float(\"cat_smooth\", 10, 100, log=True),\n            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 10),\n            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.001, 2, log=True),\n            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n            \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 8),\n            \"max_bin\": trial.suggest_int(\"max_bin\", 16, 255, log=True),\n        }\n\n        categoricals = [\n            \"shop_id\",\n            \"item_category_id\",\n            \"city_code\",\n            \"month\",\n            \"interaction_month_digital\",\n            \"interaction_month_item_category_id\",\n        ]\n        \n        categoricals = [c for c in categoricals if c in self.X_train.columns]\n        \n        early_stopping_rounds=int(1/params['learning_rate'])\n        \n        # Add a callback for pruning.\n        pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n\n        booster = lgbm.LGBMRegressor(**params)\n\n        booster.fit(\n            self.X_train,\n            self.y_train,\n            eval_set=[(self.X_valid, self.y_valid)],\n            eval_metric=[\"rmse\"],\n            callbacks=[pruning_callback],\n            verbose=False,\n            categorical_feature=categoricals,\n            early_stopping_rounds=early_stopping_rounds\n            \n        )\n\n        predictions = booster.predict(self.X_valid)\n        rmse = mean_squared_error(self.y_valid, predictions.clip(0,20), squared=False)\n\n        return rmse\n\n\nimport warnings\n\n# warnings.filterwarnings(\"ignore\", message=\"\", module=\"lightgbm\", lineno=1286)\nwarnings.filterwarnings(\"ignore\", message=\"\", module=\"lightgbm\")\n\nstudy = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=15), direction=\"minimize\")\nstudy.optimize(Objective(matrix), n_trials=100, gc_after_trial=True)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\njoblib.dump(study, \"optuna_results_21.03.2021.pk1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"finished everything!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}