{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing, utility function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A few functions are imported from a utility script\n",
    "import futuresalesutility as fu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and convert the date column in the training data to the datetime dtype to enable datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories_extra = pd.read_csv('../input/predict-future-sales-extra/item_categories_enhanced.csv')\n",
    "item_categories_extra = item_categories_extra.drop(columns=['category_name', 'supercategory', 'platform'])\n",
    "items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\n",
    "shops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n",
    "train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n",
    "test = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\n",
    "\n",
    "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correct duplicate shop names\n",
    "* Drop a few duplicate items in the training set\n",
    "* Drop shops which are not in the test set (these are only a few of these and they tend to be strange in some way, e.g. low sales, not operating for long)+\n",
    "* Drop categories 8 and 80 as these are for tickets to an annual exhibition which are not sold in the test month\n",
    "* Remove outliers and negative values for the item_cnt_day and item_price features (these are few in number but cause problems when generating some features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct shop labels\n",
    "train.loc[train.shop_id == 0, \"shop_id\"] = 57\n",
    "train.loc[train.shop_id == 1, \"shop_id\"] = 58\n",
    "train.loc[train.shop_id == 11, \"shop_id\"] = 10\n",
    "# Drop shops not in test set\n",
    "testshops = test.shop_id.unique()\n",
    "train = train.loc[train[\"shop_id\"].isin(testshops), :]\n",
    "del testshops\n",
    "# Drop duplicates\n",
    "train = train.drop_duplicates()\n",
    "# Drop categories 8 and 80\n",
    "train = train.merge(items[[\"item_id\", \"item_category_id\"]], on=\"item_id\", how=\"left\")\n",
    "train = train[~train.item_category_id.isin([8, 80])]\n",
    "# Clip outliers and  remove items with a negative sales price or item_cnt_day\n",
    "train = train.query(\"(item_price>0) & (item_cnt_day>0)\")\n",
    "train.loc[:, \"item_price\"] = train.loc[:, \"item_price\"].clip(0, train[\"item_price\"].quantile(0.9999))\n",
    "train.loc[:, \"item_cnt_day\"] = train.loc[:, \"item_cnt_day\"].clip(0, train[\"item_cnt_day\"].quantile(0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a training matrix similar to the test items by aggregating the sales to the month level and creating items for every possible combination of shops and items featured in each individual month of the training data. Additionally, concatenate test to train data to enable creation of features for the test items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testlike_train(sales_train, test=None):\n",
    "    # Create a date_block_num / item_id / shop_id index using all combinations of item_id and shop_id occurring within each date_block\n",
    "    # Optionally concatenate the test items to the end\n",
    "    indexlist = []\n",
    "    for i in sales_train.date_block_num.unique():\n",
    "        x = itertools.product(\n",
    "            [i],\n",
    "            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n",
    "            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n",
    "        )\n",
    "        indexlist.append(np.array(list(x)))\n",
    "    df = pd.DataFrame(\n",
    "        data=np.concatenate(indexlist, axis=0), columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    # Add revenue column to sales_train\n",
    "    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n",
    "    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n",
    "    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n",
    "        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n",
    "        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n",
    "    )\n",
    "\n",
    "    # Merge the grouped data with the index\n",
    "    df = df.merge(sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],)\n",
    "\n",
    "    if test is not None:\n",
    "        test[\"date_block_num\"] = 34\n",
    "        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
    "        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
    "        test[\"item_id\"] = test.item_id.astype(np.int16)\n",
    "        test = test.drop(columns=\"ID\")\n",
    "\n",
    "        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n",
    "\n",
    "    # Fill empty item_cnt entries with 0\n",
    "    df.item_cnt_month = df.item_cnt_month.fillna(0)\n",
    "    df.item_revenue_month = df.item_revenue_month.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_testlike_train(train, test)\n",
    "del(test)\n",
    "matrix = fu.reduce_mem_usage(matrix, silent=False)\n",
    "oldcols = matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering  \n",
    "In this section predictor feature columns are generated and added to the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time features\n",
    "Time-dependent features\n",
    "* Item and shop age in months\n",
    "* Binary features for new items and shops (i.e. first month of appearance)\n",
    "* Item and shop age in days\n",
    "* Number of days in the current month\n",
    "* Average item count per day in the last month - useful for new items which may not have been available for the full calendar month\n",
    "* Days since last sale of an item in each shop and all shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(m, train, correct_item_cnt_day=False):\n",
    "    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n",
    "\n",
    "    def item_shop_age_months(m):\n",
    "        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(lambda x: x - x.min())\n",
    "        # Sales tend to plateau after 12 months\n",
    "        m[\"new_item\"] = m[\"item_age\"] == 0\n",
    "        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n",
    "        m[\"shop_age\"] = m.groupby(\"shop_id\")[\"date_block_num\"].transform(lambda x: x - x.min()).astype(\"int8\")\n",
    "        m[\"new_shop\"] = m.shop_age == 0\n",
    "        m[\"new_shop\"] = m[\"new_shop\"].astype(\"int8\")\n",
    "        return m\n",
    "\n",
    "    # Add dummy values for the test month so that features are created correctly\n",
    "    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n",
    "    dummies = dummies.assign(date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,)\n",
    "    train = pd.concat([train, dummies])\n",
    "    del dummies\n",
    "\n",
    "    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n",
    "    month_last_day[~month_last_day.dt.is_month_end] = month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n",
    "    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n",
    "    month_first_day[~month_first_day.dt.is_month_start] = month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n",
    "    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n",
    "    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n",
    "    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n",
    "    first_shop_item_date = train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n",
    "\n",
    "    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\")\n",
    "\n",
    "    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n",
    "    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n",
    "    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n",
    "    m[\"item_in_shop_days\"] = m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n",
    "    m = m.drop(columns=\"item_first_sale_days\")\n",
    "    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] / m[\"item_in_shop_days\"]\n",
    "    m[\"month_length\"] = m[\"month_length\"].dt.days\n",
    "\n",
    "    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n",
    "    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n",
    "    m[\"first_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n",
    "    m[\"first_shop_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n",
    "    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n",
    "    m[\"first_item_sale_days\"] = m[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0)\n",
    "    m[\"first_shop_item_sale_days\"] = m[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0)\n",
    "\n",
    "    # Change the first sale date feature to integer format indexed from the first day in the training data\n",
    "    m[\"first_day\"] = train.date.min()\n",
    "    m[\"first_item_date\"] = (m[\"first_item_date\"] - m[\"first_day\"]).dt.days\n",
    "    m[\"first_shop_item_date\"] = (m[\"first_shop_item_date\"] - m[\"first_day\"]).dt.days\n",
    "\n",
    "    # Add days since last sale\n",
    "    def last_sale_days(matrix):\n",
    "        last_shop_item_dates = []\n",
    "        last_item_dates = []\n",
    "        for dbn in range(1, 35):\n",
    "            lsid_temp = train.query(f\"date_block_num<{dbn}\").groupby([\"shop_id\", \"item_id\"]).date.max().rename(\"last_shop_item_sale_date\").reset_index()\n",
    "            lid_temp = lsid_temp.groupby(\"item_id\").last_shop_item_sale_date.max().rename(\"last_item_sale_date\").reset_index()\n",
    "            lsid_temp[\"date_block_num\"] = dbn\n",
    "            lid_temp[\"date_block_num\"] = dbn\n",
    "            last_shop_item_dates.append(lsid_temp)\n",
    "            last_item_dates.append(lid_temp)\n",
    "        last_shop_item_dates = pd.concat(last_shop_item_dates)\n",
    "        last_item_dates = pd.concat(last_item_dates)\n",
    "        matrix = matrix.merge(last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\")\n",
    "        matrix = matrix.merge(last_item_dates, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n",
    "\n",
    "        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n",
    "            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n",
    "            m.loc[m[feat_name] > 2000, feat_name] = missingval\n",
    "            m.loc[m[feat_name].isna(), feat_name] = missingval\n",
    "            return m\n",
    "\n",
    "        matrix = days_since_last_feat(matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999)\n",
    "        matrix = days_since_last_feat(matrix, \"last_item_sale_days\", \"last_item_sale_date\", 9999)\n",
    "        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\", \"last_item_sale_date\"])\n",
    "        return matrix\n",
    "\n",
    "    m = last_sale_days(m)\n",
    "    # Month\n",
    "    m[\"month\"] = m[\"month_first_day\"].dt.month\n",
    "\n",
    "    m = m.drop(\n",
    "        columns=[\n",
    "            \"first_day\",\n",
    "            \"month_first_day\",\n",
    "            \"month_last_day\",\n",
    "            \"first_shop_date\",\n",
    "            \"first_item_date\",\n",
    "            \"item_in_shop_days\",\n",
    "            \"first_shop_item_date\",\n",
    "            \"month_length\"\n",
    "        ],\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "\n",
    "    m = item_shop_age_months(m)\n",
    "\n",
    "    if correct_item_cnt_day == True:\n",
    "        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n",
    "        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_time_features(matrix, train, False)\n",
    "print(\"Time features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally clip the corrected monthly item counts to remove the effect of outliers, but save the unclipped version for use as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(lower=0, upper=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price features  \n",
    "* Last mean price for a month in which the item was sold\n",
    "* Difference of the last price from the historical mean (calculated with an expanding window)\n",
    "* Difference of the last mean price from the first sale price in the data (proxy for the release price)\n",
    "* Difference of the last mean price from the mean for the category in the same month.  \n",
    "\n",
    "It can be assumed that item price is related to sales, both in absolute terms and relative to other items of the same category and the same item's earlier price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_price_features(matrix, train):\n",
    "    # Get mean prices per month from train dataframe\n",
    "    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n",
    "    price_features = pd.DataFrame(price_features)\n",
    "    price_features = price_features.reset_index()\n",
    "    # Calculate expanding mean and join to dataframe\n",
    "    emp = price_features.groupby(\"item_id\").item_price.expanding().mean()\n",
    "    emp = emp.reset_index(level=0)\n",
    "    price_features[\"exp_mean_price\"] = emp[\"item_price\"]\n",
    "    del emp\n",
    "    # Get first sale price for item\n",
    "    price_features[\"start_price\"] = price_features.groupby(\n",
    "        \"item_id\"\n",
    "    ).item_price.transform(lambda x: x.to_numpy()[0])\n",
    "    # Calculate  differences from start and mean prices\n",
    "    price_features[\"norm_diff_exp_mean_price\"] = (\n",
    "        price_features[\"item_price\"] - price_features[\"exp_mean_price\"]\n",
    "    ) / price_features[\"exp_mean_price\"]\n",
    "    price_features[\"norm_diff_start_price\"] = (\n",
    "        price_features[\"item_price\"] - price_features[\"start_price\"]\n",
    "    ) / price_features[\"start_price\"]\n",
    "    # Calculate normalized differences from start and mean prices\n",
    "    price_features[\"norm_diff_exp_mean_price\"] = (\n",
    "        price_features[\"item_price\"] - price_features[\"exp_mean_price\"]\n",
    "    ) / price_features[\"exp_mean_price\"]\n",
    "    # Calculate normalized differenced from mean category price per month\n",
    "    price_features = price_features.merge(\n",
    "        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n",
    "    )\n",
    "    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n",
    "        [\"date_block_num\", \"item_category_id\"]\n",
    "    )[\"item_price\"].transform(lambda x: (x - x.mean()) / x.mean())\n",
    "    # Retain only the necessary features\n",
    "    price_features = price_features[\n",
    "        [\n",
    "            \"date_block_num\",\n",
    "            \"item_id\",\n",
    "            \"item_price\",\n",
    "            \"norm_diff_exp_mean_price\",\n",
    "            \"norm_diff_start_price\",\n",
    "            \"norm_diff_cat_price\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        \"item_price\",\n",
    "        \"norm_diff_exp_mean_price\",\n",
    "        \"norm_diff_start_price\",\n",
    "        \"norm_diff_cat_price\",\n",
    "    ]\n",
    "    newnames = [\"last_\" + f for f in features]\n",
    "    aggs = {f: \"last\" for f in features}\n",
    "    renames = {f: \"last_\" + f for f in features}\n",
    "\n",
    "    features = []\n",
    "    for dbn in range(1, 35):\n",
    "        f_temp = (\n",
    "            price_features.query(f\"date_block_num<{dbn}\")\n",
    "            .groupby(\"item_id\")\n",
    "            .agg(aggs)\n",
    "            .rename(columns=renames)\n",
    "        )\n",
    "        f_temp[\"date_block_num\"] = dbn\n",
    "        features.append(f_temp)\n",
    "    features = pd.concat(features).reset_index()\n",
    "\n",
    "    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = add_price_features(matrix, train)\n",
    "del(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features provided with the data, e.g. item category, and custom categories extracted from category and shop names, e.g. \"video games\", \"music\", \"PS4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = matrix.merge(items.drop(columns='item_name'), on='item_id', how='left')\n",
    "matrix = matrix.merge(item_categories_extra, on='item_category_id', how='left')\n",
    "del(item_categories_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City that the shop is located in\n",
    "(from https://www.kaggle.com/dlarionov/feature-engineering-xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_city_codes(matrix, shops):\n",
    "    shops.loc[\n",
    "        shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n",
    "    ] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n",
    "    shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
    "    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n",
    "    shop_labels = shops[[\"shop_id\", \"city_code\"]]\n",
    "    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n",
    "    return matrix\n",
    "\n",
    "matrix = add_city_codes(matrix, shops)\n",
    "del(shops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item name groups discovered by name matching with fuzzywuzzy\n",
    "\n",
    "Items in the items table are ordered alphabetically according to the item_name field, so that similar items are generally listed next to each other. This ordering can be used to help group related items together, which could perhaps help share information between items.  \n",
    "\n",
    "The following cell groups similar items together by using the sting matching algorithm fuzzywuzzy (https://github.com/seatgeek/fuzzywuzzy) is used to measure the similarity of each item to the item preceding it in the items table, then grouping adjacent items into groups unless the similarity score is below a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.query(\"item_id>3564\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def prune_below_quantile(matrix, feature, prune_quantile):\n",
    "    # replace low-occurence words / artists by a category marker\n",
    "    vc = matrix[feature].value_counts().rename(\"vcs\")\n",
    "    matrix = matrix.merge(vc, left_on=feature, right_index=True, how=\"left\")\n",
    "    prune_mask = matrix.vcs <= matrix[\"vcs\"].quantile(prune_quantile)\n",
    "    matrix.loc[prune_mask, feature] = -matrix.loc[prune_mask, \"item_category_id\"]\n",
    "    matrix = matrix.drop(columns=\"vcs\")\n",
    "    matrix[feature] -= matrix[feature].min()\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def add_item_name_groups(matrix, items, sim_thresh, prune_quantile, feature_name=\"item_name_group\", artist_names=True):\n",
    "    def partialmatchgroups(items, sim_thresh=sim_thresh):\n",
    "        def strip_brackets(string):\n",
    "            string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "            string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "            return string\n",
    "\n",
    "        items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "        items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "\n",
    "        def partialcompare(s):\n",
    "            return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "\n",
    "        items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "        # Assign groups\n",
    "        grp = 0\n",
    "        for i in range(items.shape[0]):\n",
    "            items.loc[i, \"partialmatchgroup\"] = grp\n",
    "            if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "                grp += 1\n",
    "        items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "        return items\n",
    "\n",
    "    items = partialmatchgroups(items)\n",
    "    items = items.rename(columns={\"partialmatchgroup\": feature_name})\n",
    "    items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n",
    "\n",
    "    items[feature_name] = items[feature_name].apply(str)\n",
    "    items[feature_name] = items[feature_name].factorize()[0]\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "\n",
    "    if prune_quantile > 0:  # replace low-occurence words / artists by a category marker\n",
    "        matrix = prune_below_quantile(matrix, feature_name, prune_quantile)\n",
    "    matrix[feature_name] = fu.reduce_mem_usage(matrix[feature_name])\n",
    "    return matrix, items\n",
    "\n",
    "\n",
    "matrix, items = add_item_name_groups(matrix, items, 65, 0.0, artist_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music artist extraction or first word of item name  \n",
    "\n",
    "The \"item_name\" field of items in the music categories typically begin with the artist's name marked with one of three patterns: either all uppercase, separated from the release title by a doublespace, or separated by dot-space (. ). Regex expressions are used to extract artist names from music categories using these rules. For non-music categories, the first word in the name is extracted instead.  \n",
    "\n",
    "This feature adds a more general grouping to the preceding item name group features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def add_first_word_features(\n",
    "    matrix, items=items, fillna_value=-99, feature_name=\"artist_name_or_first_word\",\n",
    "    prune_quantile=0.25\n",
    "):\n",
    "    # This extracts artist names for music categories and adds them as a feature.\n",
    "    def extract_artist(st):\n",
    "\n",
    "        st = st.strip()\n",
    "        if st.startswith(\"V/A\"):\n",
    "            artist = \"V/A\"\n",
    "        elif st.startswith(\"СБ\"):\n",
    "            artist = \"СБ\"\n",
    "        else:\n",
    "            # Retrieves artist names using the double space or all uppercase pattern\n",
    "            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n",
    "            match_dubspace = mus_artist_dubspace.match(st)\n",
    "            mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n",
    "            match_capsonly = mus_artist_capsonly.match(st)\n",
    "            candidates = [match_dubspace, match_capsonly]\n",
    "            candidates = [m[0] for m in candidates if m is not None]\n",
    "            # Sometimes one of the patterns catches some extra words so choose the shortest one\n",
    "            if len(candidates):\n",
    "                artist = min(candidates, key=len)\n",
    "            else:\n",
    "                # If neither of the previous patterns found something, use the dot-space pattern\n",
    "                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n",
    "                match = mus_artist_dotspace.match(st)\n",
    "                if match:\n",
    "                    artist = match[0]\n",
    "                else:\n",
    "                    artist = \"\"\n",
    "        artist = artist.upper()\n",
    "        artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n",
    "        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n",
    "        artist = artist.strip()\n",
    "        return artist\n",
    "    \n",
    "    all_stopwords = stopwords.words(\"russian\")\n",
    "    all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "    def first_word(string):\n",
    "        # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n",
    "        string = re.sub(r\"[^\\w\\s]\", \"\", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        tokens = string.lower().split()\n",
    "        tokens = [t for t in tokens if t not in all_stopwords]\n",
    "        token = tokens[0] if len(tokens) > 0 else \"\"\n",
    "        return token\n",
    "\n",
    "    music_categories = [55, 56, 57, 58, 59, 60]\n",
    "    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[items.item_category_id.isin(music_categories), \"item_name\"].apply(\n",
    "        extract_artist\n",
    "    )\n",
    "\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other music\"\n",
    "\n",
    "    items.loc[~items.item_category_id.isin(music_categories), feature_name] = items.loc[~items.item_category_id.isin(music_categories), \"item_name\"].apply(\n",
    "        first_word\n",
    "    )\n",
    "\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other non-music\"\n",
    "\n",
    "    items[feature_name] = items[feature_name].factorize(na_sentinel=fillna_value)[0]\n",
    "\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\",)\n",
    "    if fillna_value is not None:\n",
    "        matrix[feature_name] = matrix[feature_name].fillna(fillna_value)\n",
    "    \n",
    "    if prune_quantile>0:  # replace low-occurence words / artists by a category marker\n",
    "        matrix = prune_below_quantile(matrix, feature_name, prune_quantile)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "matrix =  add_first_word_features(\n",
    "    matrix, items=items, fillna_value=-99, feature_name=\"artist_name_or_first_word\",\n",
    "    prune_quantile=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item name length as a feature\n",
    "Surprisingly predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_item_name(string):\n",
    "    # Removes bracketed terms, special characters and extra whitespace\n",
    "    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "items[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\n",
    "items['item_name_length'] = items.item_name.apply(len)\n",
    "matrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Created name features\")\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shop sales profile clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def shop_cluster_features(matrix, n_clusters=5):\n",
    "    start_month=20\n",
    "    end_month=32\n",
    "    pt = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month} & shop_id!=36 & shop_id!=55 & shop_id!=12\")\n",
    "    # mx = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month} & shop_id!=36\")\n",
    "    pt = pt.pivot_table(values='item_cnt_month', columns='shop_id', index=['item_name_group'], fill_value=0, aggfunc='mean')\n",
    "    pt = pt.transpose()\n",
    "    pca = PCA(n_components=4)\n",
    "    components = pca.fit_transform(pt)\n",
    "    components = pd.DataFrame(components)\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "    labels = clusterer.fit_predict(components)\n",
    "    x = components[0]\n",
    "    y = components[1]\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.scatterplot(x=x, y=y, hue=labels, palette=sns.color_palette(\"hls\",n_clusters), ax=ax)\n",
    "    plt.title('Shop clusters')\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    for i, txt in enumerate(pt.index.to_list()):\n",
    "        ax.annotate(str(txt), (x[i], y[i]))\n",
    "    shopgroups = {}\n",
    "    for i, s in enumerate(pt.index):\n",
    "        shopgroups[s] =  labels[i]\n",
    "    shopgroups[36] = shopgroups[37]\n",
    "    shopgroups[55] = max(shopgroups.values()) + 1\n",
    "    shopgroups[12] = max(shopgroups.values()) + 1\n",
    "    return shopgroups\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_group_dict = shop_cluster_features(matrix, n_clusters=5)\n",
    "matrix['shop_group'] = matrix['shop_id'].map(shop_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory\n",
    "matrix.to_pickle(\"matrixcheckpoint_1.pkl\")\n",
    "print(\"Saved matrixcheckpoint 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"matrixcheckpoint_1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage change in an aggregate feature over a specified period  \n",
    "e.g. change in total shop revenue compared to the previous month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pct_change(\n",
    "    df, group_feats, quantity=\"item_cnt_month\", agg_function=\"mean\", periods=1, lag=1, clip_value=None,\n",
    "):\n",
    "    periods = fu.list_if_not(periods, int)\n",
    "    group_feats = fu.list_if_not(group_feats)\n",
    "    group_feats_full = [\"date_block_num\"] + group_feats\n",
    "    idx = pd.MultiIndex.from_product([df[col].unique() for col in group_feats_full], names=group_feats_full)\n",
    "    template = pd.DataFrame(index=idx)\n",
    "    template = template.sort_index()\n",
    "    aggs = df.groupby(group_feats_full)[quantity].agg(agg_function).fillna(value=0)\n",
    "    template = template.merge(aggs, on=group_feats_full, how=\"left\")\n",
    "    template = fu.reduce_mem_usage(template, float_dtype=\"float32\")\n",
    "    for period in periods:\n",
    "        feat_name = \"_\".join(group_feats + [quantity] + [agg_function] + [\"delta\"] + [str(period)])\n",
    "        print(f\"Adding feature {feat_name}\")\n",
    "        feature_series = (\n",
    "            template.groupby(group_feats)[quantity]\n",
    "            .transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\"))\n",
    "#             .fillna(value=0)\n",
    "            .rename(feat_name)\n",
    "        )\n",
    "        if clip_value is not None:\n",
    "            feature_series = feature_series.clip(lower=-clip_value, upper=clip_value)\n",
    "        df = fu.add_lag_feature(df, feature_series, group_feats, lag=lag)\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_pct_change(matrix, [\"shop_id\", \"item_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"shop_id\", \"item_category_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"shop_id\", \"item_name_group\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_name_group\"], \"item_cnt_month\", clip_value=3)\n",
    "# Mean item price change\n",
    "matrix = add_pct_change(matrix, [\"item_id\"], \"last_item_price\", clip_value=3, lag=0)\n",
    "# Delta 1 features lagged by 12 months, intended to capture seasonal trends\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\n",
    "matrix = add_pct_change(matrix, [\"shop_id\", \"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\n",
    "gc.collect()\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling mean features\n",
    "Sales from previous months are a good predictor of future sales, but chance fluctuations mean that an average of several previous months may be more reliable than counts from a single month. Pandas has several windowing functions for time series built in, 3 of which are demonstrated below : expanding (all previous timepoints), rolling (a fixed number of previous timepoints) and exponential (a weighted window with weights which decrease with time distance before the current point). These are compared below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id = 16\n",
    "item_id = 482\n",
    "im = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\n",
    "im['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\n",
    "im['expanding mean'] = im['item_cnt_month'].expanding().mean()\n",
    "im['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\n",
    "im = im.set_index('date_block_num')\n",
    "ax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_stat(\n",
    "    matrix, features, window=12, kind=\"rolling\", argfeat=\"item_cnt_month\", aggfunc=\"mean\", rolling_aggfunc=\"mean\", dtype=\"float16\", reshape_source=True\n",
    "):\n",
    "    # General purpose windowed aggregation function\n",
    "    def rolling_stat(matrix, source, feats, feat_name, window=12, argfeat=\"item_cnt_month\", aggfunc=\"mean\", dtype=\"float16\"):\n",
    "        # Calculate a statistic on a windowed section of a source table grouping on specific features\n",
    "        store = []\n",
    "        for i in range(2, 35):\n",
    "            mes = (\n",
    "                source[source.date_block_num.isin(range(min([i - window, 0]), i))]\n",
    "                .groupby(feats)[argfeat]\n",
    "                .agg(aggfunc)\n",
    "                .astype(dtype)\n",
    "                .rename(feat_name)\n",
    "                .reset_index()\n",
    "            )\n",
    "            mes[\"date_block_num\"] = i\n",
    "            store.append(mes)\n",
    "        store = pd.concat(store)\n",
    "        matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n",
    "        return matrix\n",
    "    \n",
    "    if (reshape_source == True) or (kind == \"ewm\"):\n",
    "        source = matrix.pivot_table(index=features + [\"date_block_num\"], values=argfeat, aggfunc=aggfunc, fill_value=0, dropna=False).astype(dtype)\n",
    "        for g in features:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            source.loc[source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat] = float(\"nan\")\n",
    "            del source[\"firsts\"]\n",
    "        source = source.reset_index()\n",
    "    else:\n",
    "        source = matrix\n",
    "\n",
    "    if kind == \"rolling\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(matrix, source, features, feat_name, window=window, argfeat=argfeat, aggfunc=rolling_aggfunc, dtype=dtype)\n",
    "    elif kind == \"expanding\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(matrix, source, features, feat_name, window=100, argfeat=argfeat, aggfunc=aggfunc, dtype=dtype)\n",
    "    elif kind == \"ewm\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        source[feat_name] = source.groupby(features)[argfeat].ewm(halflife=window, min_periods=1).agg(rolling_aggfunc).to_numpy(dtype=dtype)\n",
    "        del source[argfeat]\n",
    "#         source = source.reset_index()\n",
    "        source[\"date_block_num\"] += 1\n",
    "        return matrix.merge(source, on=[\"date_block_num\"] + features, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement rolling mean / median / minimum / maximum features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versions with default target\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\"],  kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\"],  kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\"],  kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\"],  kind=\"ewm\", window=1)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"item_id\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"artist_name_or_first_word\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"item_category_id\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"item_id\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"artist_name_or_first_word\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"item_category_id\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"item_category_id\"], kind=\"ewm\", window=1)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"artist_name_or_first_word\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"item_category_id\", \"item_age\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"artist_name_or_first_word\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"item_category_id\", \"item_age\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"artist_name_or_first_word\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"item_category_id\", \"new_item\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"item_category_id\", \"new_item\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_id\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_id\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_id\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_name_group\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"artist_name_or_first_word\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_category_id\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_id\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_name_group\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"artist_name_or_first_word\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_category_id\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_name_group\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"artist_name_or_first_word\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_category_id\"], kind=\"ewm\", window=1)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_name_group\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"artist_name_or_first_word\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_category_id\", \"item_age\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_name_group\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"artist_name_or_first_word\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_category_id\", \"item_age\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_name_group\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"artist_name_or_first_word\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_category_id\", \"new_item\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_name_group\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"city_code\", \"item_category_id\", \"new_item\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_id\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_name_group\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"artist_name_or_first_word\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_category_id\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_id\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_name_group\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"artist_name_or_first_word\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_category_id\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_name_group\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"artist_name_or_first_word\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_category_id\"], kind=\"ewm\", window=1)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_name_group\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"artist_name_or_first_word\", \"item_age\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_category_id\", \"item_age\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_name_group\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"artist_name_or_first_word\", \"item_age\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_category_id\", \"item_age\"], kind=\"expanding\")\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_name_group\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"artist_name_or_first_word\", \"new_item\"], window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_category_id\", \"new_item\"], window=12)\n",
    "\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_name_group\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_group\", \"item_category_id\", \"new_item\"], kind=\"expanding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n",
    "matrix.to_pickle(\"matrixcheckpoint_2.pkl\")\n",
    "print(\"Saved matrixcheckpoint 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block calculates windowed mean sales features with day resolution accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summed sales & accurate windowed mean sales per day features\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_id\"], aggregation=\"sum\", rolling_aggregation=\"sum\", kind=\"rolling\", window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"item_id\"], aggregation=\"sum\", rolling_aggregation=\"sum\", kind=\"rolling\", window=12)\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_id\"], aggregation=\"sum\", rolling_aggregation=\"sum\", kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"item_id\"], aggregation=\"sum\", rolling_aggregation=\"sum\", kind=\"expanding\")\n",
    "matrix[\"1year\"] = 365\n",
    "matrix[\"shop_id_item_id_day_mean_win_12\"] = (\n",
    "    matrix[\"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"] / matrix[[\"first_item_sale_days\", \"shop_open_days\", \"1year\"]].min(axis=1)\n",
    ")\n",
    "matrix[\"item_id_day_mean_win_12\"] = (\n",
    "    matrix[\"item_id_item_cnt_month_sum_rolling_sum_win_12\"] / matrix[[\"first_item_sale_days\", \"1year\"]].min(axis=1)\n",
    ")\n",
    "matrix[\"shop_id_item_id_day_mean_expanding\"] = (\n",
    "    matrix[\"shop_id_item_id_item_cnt_month_sum_expanding_sum\"] / matrix[[\"first_item_sale_days\", \"shop_open_days\"]].min(axis=1)\n",
    ")\n",
    "matrix[\"item_id_day_mean_expanding\"] = (matrix[\"item_id_item_cnt_month_sum_expanding_sum\"] / matrix[[\"first_item_sale_days\"]].min(axis=1))\n",
    "matrix.loc[\n",
    "    matrix.new_item == True,\n",
    "    [\"shop_id_item_id_day_mean_win_12\", \"item_id_day_mean_win_12\", \"shop_id_item_id_day_mean_expanding\", \"item_id_day_mean_expanding\"],\n",
    "] = float('nan')\n",
    "del matrix[\"1year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding sum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item name group expanding sum\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\"], aggregation=\"sum\", rolling_aggregation=\"sum\", kind=\"expanding\")\n",
    "matrix = add_rolling_ME(matrix, [\"item_name_group\"], aggregation=\"sum\", rolling_aggregation=\"sum\", kind=\"expanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revenue features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_rolling_ME(matrix, [\"item_id\"], window=12, target_feature=\"item_revenue_month\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\"], window=12, target_feature=\"item_revenue_month\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_id\"], window=12, target_feature=\"item_revenue_month\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_name_group\"], window=12, target_feature=\"item_revenue_month\")\n",
    "matrix = add_rolling_ME(matrix, [\"shop_id\", \"item_category_id\"], window=12, target_feature=\"item_revenue_month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagged features  \n",
    "Values for the same shop-item combination from the previous month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lag_feature(matrix, lag_feature, lags):\n",
    "    for lag in lags:\n",
    "        newname = lag_feature + f'_lag_{lag}'\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        targetseries = matrix.loc[:,['date_block_num', 'item_id', 'shop_id'] + [lag_feature]]\n",
    "        targetseries['date_block_num'] += lag\n",
    "        targetseries = targetseries.rename(columns={lag_feature: newname})\n",
    "        matrix = matrix.merge(targetseries, on=['date_block_num', 'item_id', 'shop_id'], how='left')\n",
    "        matrix[newname] = fu.reduce_mem_usage(matrix[newname])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1,2,3])\n",
    "matrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1])\n",
    "matrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\n",
    "gc.collect()\n",
    "print(\"Lag features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean encodings\n",
    "The mean value of a target feature for each level of a categorical feature or combination of categorical features, lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_apply_ME(matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\"):\n",
    "    grouping_fields = fu.list_if_not(grouping_fields)\n",
    "    for lag in lags:\n",
    "        newname = \"_\".join(grouping_fields + [\"ME\"] + [f\"lag_{lag}\"])\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        me_series = matrix.groupby([\"date_block_num\"] + grouping_fields)[target].agg(\"mean\").rename(newname).reset_index()\n",
    "        me_series[\"date_block_num\"] += lag\n",
    "        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n",
    "        matrix[newname] = fu.reduce_mem_usage(matrix[newname])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_apply_ME(matrix, [\"item_id\"], [1, 2, 3])\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"artist_name_or_first_word\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_category_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_id\", \"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_id\", \"artist_name_or_first_word\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_id\", \"item_category_id\"], [1, 12], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_group\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_group\", \"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_group\", \"item_category_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_category_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratios between recent sales and rolling 12 month means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_id_ME_lag_1\"] / matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n",
    ")\n",
    "matrix[\"shop_id_item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_cnt_day_avg_lag_1\"] / matrix[\"shop_id_item_id_day_mean_win_12\"]\n",
    ")\n",
    "matrix[\"item_category_id_item_cnt_lag_1_12_ratio\"] = (\n",
    "    matrix[\"item_category_id_ME_lag_1\"] / matrix[\"item_category_id_item_cnt_month_mean_rolling_mean_win_12\"]\n",
    ")\n",
    "matrix[\"shop_id_item_category_id_item_cnt_lag_1_12_ratio\"] = (\n",
    "    matrix[\"shop_id_item_category_id_ME_lag_1\"]\n",
    "    / matrix[\"shop_id_item_category_id_item_cnt_month_mean_rolling_mean_win_12\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n",
    "matrix.to_pickle(\"matrixcheckpoint_3.pkl\")\n",
    "print(\"Saved matrixcheckpoint 3\")\n",
    "gc.collect()\n",
    "print(\"Mean encoding features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle('matrixcheckpoint_3.pkl')\n",
    "oldcols = matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_feature(matrix, features):\n",
    "    feature_name = '_'.join(['interaction'] + features)\n",
    "    matrix[feature_name] = matrix[features[0]].apply(str)\n",
    "    for feature in features[1:]:\n",
    "        matrix[feature_name] = matrix[feature_name] + '_' +  matrix[feature].apply(str)\n",
    "    matrix[feature_name] = fu.reduce_mem_usage(matrix[feature_name], allow_categorical=False)\n",
    "    return matrix\n",
    "\n",
    "# Month category interactions\n",
    "matrix = interaction_feature(matrix, [\"month\", \"item_category_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor data leak features  \n",
    "i.e. counts of new and unique items in the current month  \n",
    "This counts the number of unique and new items sold in the current month and also category, which could have a positive relationship to sales. As this can also be calculated for the test set (assuming that the set of test items is the set of items that were sold in the test month) this is a kind of data leak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_item_features(matrix):\n",
    "    # Adds the number of unique and new items per month and month-category, and these values as a proportion\n",
    "    unique_id_features = matrix.groupby([\"date_block_num\", \"item_category_id\"]).item_id.nunique().rename(\"unique_items_cat\").reset_index()\n",
    "    unique_id = unique_id_features.groupby(\"date_block_num\").unique_items_cat.sum().rename(\"unique_items_month\").reset_index()\n",
    "    unique_id_features = unique_id_features.merge(unique_id, how=\"left\", on=\"date_block_num\",)\n",
    "    unique_id_features[\"cat_items_proportion\"] = unique_id_features[\"unique_items_cat\"].div(unique_id_features[\"unique_items_month\"])\n",
    "    new_items_cat = (\n",
    "        matrix.loc[matrix.new_item == 1, [\"date_block_num\", \"item_id\", \"item_category_id\"]]\n",
    "        .groupby([\"date_block_num\", \"item_category_id\"])\n",
    "        .item_id.nunique()\n",
    "        .rename(\"new_items_cat\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    unique_id_features = unique_id_features.merge(new_items_cat, on=[\"date_block_num\", \"item_category_id\"], how=\"left\",)\n",
    "    new_items_month = unique_id_features.groupby(\"date_block_num\").new_items_cat.sum().rename(\"new_items_month\").reset_index()\n",
    "    unique_id_features = unique_id_features.merge(new_items_month, on=\"date_block_num\", how=\"left\")\n",
    "    unique_id_features[\"cat_items_new_proportion\"] = unique_id_features[\"new_items_cat\"] / unique_id_features[\"unique_items_cat\"]\n",
    "    matrix = matrix.merge(unique_id_features, on=[\"date_block_num\", \"item_category_id\"], how=\"left\")\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = add_unique_item_features(matrix)\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive words in item_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def name_token_feats(matrix, items, k = 50, item_n_threshold=5):\n",
    "    def name_correction(st):\n",
    "        st = re.sub(r\"[^\\w\\s]\", \"\", st)\n",
    "        st = re.sub(r\"\\s{2,}\", \" \", st)\n",
    "        st = st.lower().strip()\n",
    "        return st\n",
    "    items[\"item_name_clean\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n",
    "    def create_item_id_bow_matrix(items):\n",
    "        all_stopwords = stopwords.words(\"russian\")\n",
    "        all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        vectorizer = CountVectorizer(stop_words=all_stopwords)\n",
    "        X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n",
    "        X = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "        print(f\"{len(vectorizer.vocabulary_)} words\")\n",
    "        featuremap = {col: \"word_\" + token for col, token in zip(range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names())}\n",
    "        X = X.rename(columns=featuremap)\n",
    "        return X\n",
    "\n",
    "    items_bow = create_item_id_bow_matrix(items)\n",
    "    items_bow = items_bow.clip(0, 1)\n",
    "    # Drop tokens which are in few item names\n",
    "    items_bow = items_bow.drop(columns=items_bow.columns[(items_bow.sum(axis=0) < item_n_threshold)])\n",
    "    mxbow = matrix[['date_block_num', 'item_id', 'item_cnt_month']].query(\"date_block_num<33\")\n",
    "    mxbow = mxbow.merge(items_bow, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "\n",
    "    X = mxbow.drop(columns=['date_block_num', 'item_id', 'item_cnt_month'])\n",
    "    y = mxbow['item_cnt_month']\n",
    "\n",
    "    selektor = SelectKBest(f_regression, k=k)\n",
    "    selektor.fit(X,y)\n",
    "    tokencols = X.columns[selektor.get_support()]\n",
    "    return items_bow[tokencols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frame = name_token_feats(matrix, items, k=20)\n",
    "matrix = matrix.merge(word_frame, left_on='item_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix.to_pickle(\"checkpoint_final_with_nans.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# dirlist = os.listdir()\n",
    "# for f in dirlist:\n",
    "#     if f[:16] == 'matrixcheckpoint':\n",
    "#         os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = pd.read_pickle(\"matrixcheckpoint_final.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train, validation, test sets from feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_x_y(\n",
    "#     matrix, test_month, keep_from_month=3, new_target=None\n",
    "# ):\n",
    "#     if new_target is not None:\n",
    "#         matrix[\"item_cnt_month\"] = matrix.loc[:, new_target]\n",
    "#         matrix = matrix.drop(columns=new_target)\n",
    "\n",
    "#     def split_train_test(matrix, test_month=33):\n",
    "#         # Split the matrix into train and test sets.\n",
    "#         test = matrix.loc[matrix.date_block_num==test_month, :]\n",
    "#         train = matrix.loc[matrix.date_block_num < test_month, :]\n",
    "#         return train, test\n",
    "\n",
    "#     def xysplit(matrix):\n",
    "#         # Split a train and test set into into x and y sets, with item_cnt as the target y variable\n",
    "#         y = matrix.item_cnt_month\n",
    "#         X = matrix.drop(columns=[\"item_cnt_month\"])\n",
    "#         return (X, y)\n",
    "\n",
    "#     matrix = matrix.drop(\n",
    "#         columns=[\n",
    "#             \"item_revenue_month\",\n",
    "#             \"item_price\",\n",
    "#             \"item_cnt_month_original\",\n",
    "#             \"item_cnt_month_unclipped\",\n",
    "#             \"item_cnt_day_avg\",\n",
    "#             \"new_item\",\n",
    "#             \"new_shop\",\n",
    "#             \"item_age\",\n",
    "#             \"shop_age\",\n",
    "#             \"digital\",\n",
    "#             \"first_item_date\",\n",
    "#         ],\n",
    "#         errors=\"ignore\",\n",
    "#     )\n",
    "\n",
    "#     train, test = split_train_test(matrix, test_month)\n",
    "#     train = train[train.date_block_num >= keep_from_month]\n",
    "#     X_train, y_train = xysplit(train)\n",
    "#     X_test, y_test = xysplit(test)\n",
    "#     return (X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def censor_lag_features(matrix, candidate_features, replacement_val = 9999):\n",
    "#     # replace any lag features which are invalid due to the shop / item age being less than the lag\n",
    "#     def item_lag_feats(lag):\n",
    "#         return [\n",
    "#             f\n",
    "#             for f in candidate_features\n",
    "#             if f\"lag_{lag}\" in f\n",
    "#             and \"item\" in f\n",
    "#             and f[:4] != \"shop\"\n",
    "#             and \"category\" not in f\n",
    "#             and \"city\" not in f\n",
    "#             and \"minus\" not in f\n",
    "#             and \"plus\" not in f\n",
    "#             and \"sim\" not in f\n",
    "#         ]\n",
    "\n",
    "#     def shop_lag_feats(lag):\n",
    "#         return [\n",
    "#             f\n",
    "#             for f in candidate_features\n",
    "#             if f\"lag_{lag}\" in f\n",
    "#             and \"shop\" in f\n",
    "#             and \"category\" not in f\n",
    "#             and \"city\" not in f\n",
    "#             and \"minus\" not in f\n",
    "#             and \"plus\" not in f\n",
    "#             and \"sim\" not in f\n",
    "#         ]\n",
    "\n",
    "#     lags = range(1,13)\n",
    "#     for lag in lags:\n",
    "#         lag_feats = shop_lag_feats(lag)\n",
    "#         matrix.loc[matrix.shop_age < lag, lag_feats] = replacement_val\n",
    "#         lag_feats = item_lag_feats(lag)\n",
    "#         matrix.loc[matrix.item_age < lag, lag_feats] = replacement_val\n",
    "#     return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# import numpy as np\n",
    "# import optuna\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "\n",
    "# import lightgbm as lgbm\n",
    "\n",
    "# def fit_booster(matrix, params=None, test_run = False, categoricals=[], dropcols=[]):\n",
    "#     # Regular booster fitting function\n",
    "#     if params is None:\n",
    "#         params = {\n",
    "#             \"boosting_type\": \"gbdt\",\n",
    "#             \"device_type\": \"cpu\",\n",
    "#             \"n_jobs\": 11,\n",
    "#             \"learning_rate\": 0.1,\n",
    "#             \"n_estimators\": 2000,\n",
    "#         }\n",
    "\n",
    "#     matrix = censor_lag_features(matrix, matrix.columns, replacement_val=9999)\n",
    "\n",
    "#     if test_run:\n",
    "#         test_month = 34\n",
    "#         early_stopping_rounds=None\n",
    "#     else:\n",
    "#         test_month = 33\n",
    "#         early_stopping_rounds=int(1/params['learning_rate'])\n",
    "    \n",
    "#     X_train, y_train, X_test, y_test = train_test_x_y(\n",
    "#         matrix.drop(columns=dropcols), test_month=test_month, keep_from_month=2,\n",
    "#     )\n",
    "\n",
    "#     if test_run:\n",
    "#         eval_set=[(X_train, y_train)]\n",
    "#     else:\n",
    "#         eval_set=[(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "#     booster = lgbm.LGBMRegressor(**params)\n",
    "\n",
    "#     booster.fit(\n",
    "#         X_train,\n",
    "#         y_train,\n",
    "#         eval_set=eval_set,\n",
    "#         eval_metric=[\"rmse\"],\n",
    "#         verbose=10,\n",
    "#         categorical_feature=categoricals,\n",
    "#         early_stopping_rounds=early_stopping_rounds\n",
    "#     )\n",
    "\n",
    "#     if test_run:\n",
    "#         X_test['item_cnt_month'] = booster.predict(X_test)\n",
    "#         return booster, X_test\n",
    "#     else:\n",
    "#         return booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "\n",
    "# for k in [\"shop_id_categorical\", \"interaction_month_digital_categorical\", \"city_id_categorical\", \"interaction_month_item_category_id_categorical\"]:\n",
    "#     best_params.pop(k, None)\n",
    "\n",
    "# other_params = {\n",
    "#     \"boosting_type\": \"gbdt\",\n",
    "#     \"device_type\": \"cpu\",\n",
    "#     \"n_jobs\": 11,\n",
    "#     \"silent\": True,\n",
    "#     \"n_estimators\": 10000,\n",
    "#     \"learning_rate\": 0.1,\n",
    "#     \"bagging_seed\": 3,\n",
    "#     \"subsample_for_bin\": 300000,\n",
    "#     \"max_depth\": -1,\n",
    "#     \"min_data_in_bin\": 1,\n",
    "# }\n",
    "\n",
    "# best_params = {**best_params, **other_params}\n",
    "\n",
    "# categoricals = [\n",
    "#     \"item_category_id\",\n",
    "#     \"month\",\n",
    "#     \"artist_name_or_first_word\",\n",
    "#     \"interaction_month_digital\",\n",
    "#     \"interaction_month_item_category_id\",\n",
    "# ]\n",
    "\n",
    "# booster = fit_booster(matrix, params=best_params, test_run = False, categoricals=categoricals, dropcols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = lgbm.plot_importance(booster, figsize=(10,40), height=0.5, importance_type=\"gain\", ignore_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"finished everything!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
