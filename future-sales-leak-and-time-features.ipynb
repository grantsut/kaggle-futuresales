{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053912,
     "end_time": "2021-04-28T18:11:27.348187",
     "exception": false,
     "start_time": "2021-04-28T18:11:27.294275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data loading and preprocessing, utility function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049686,
     "end_time": "2021-04-28T18:11:27.448493",
     "exception": false,
     "start_time": "2021-04-28T18:11:27.398807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.032316,
     "end_time": "2021-04-28T18:11:28.530522",
     "exception": false,
     "start_time": "2021-04-28T18:11:27.498206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# A few functions are imported from a utility script\n",
    "import futuresalesutility as fu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049382,
     "end_time": "2021-04-28T18:11:28.630332",
     "exception": false,
     "start_time": "2021-04-28T18:11:28.580950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load the data and convert the date column in the training data to the datetime dtype to enable datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.762579,
     "end_time": "2021-04-28T18:11:32.444498",
     "exception": false,
     "start_time": "2021-04-28T18:11:28.681919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "item_categories_extra = pd.read_csv(\n",
    "    \"../input/predict-future-sales-extra/item_categories_enhanced.csv\"\n",
    ")\n",
    "item_categories_extra = item_categories_extra.drop(\n",
    "    columns=[\"category_name\", \"supercategory\", \"platform\"]\n",
    ")\n",
    "items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\n",
    "shops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n",
    "train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n",
    "test = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\n",
    "\n",
    "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049628,
     "end_time": "2021-04-28T18:11:32.545881",
     "exception": false,
     "start_time": "2021-04-28T18:11:32.496253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048834,
     "end_time": "2021-04-28T18:11:32.643753",
     "exception": false,
     "start_time": "2021-04-28T18:11:32.594919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Correct duplicate shop names\n",
    "* Drop a few duplicate items in the training set\n",
    "* Drop shops which are not in the test set (these are only a few of these and they tend to be strange in some way, e.g. low sales, not operating for long)+\n",
    "* Drop categories 8 and 80 as these are for tickets to an annual exhibition which are not sold in the test month\n",
    "* Remove outliers and negative values for the item_cnt_day and item_price features (these are few in number but cause problems when generating some features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.482032,
     "end_time": "2021-04-28T18:11:34.177371",
     "exception": false,
     "start_time": "2021-04-28T18:11:32.695339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Correct shop labels\n",
    "# train.loc[train.shop_id == 0, \"shop_id\"] = 57\n",
    "# train.loc[train.shop_id == 1, \"shop_id\"] = 58\n",
    "# train.loc[train.shop_id == 11, \"shop_id\"] = 10\n",
    "# # Drop shops not in test set\n",
    "# testshops = test.shop_id.unique()\n",
    "# train = train.loc[train[\"shop_id\"].isin(testshops), :]\n",
    "# del testshops\n",
    "# # Drop duplicates\n",
    "# train = train.drop_duplicates()\n",
    "# # Drop categories 8 and 80\n",
    "# train = train.merge(items[[\"item_id\", \"item_category_id\"]], on=\"item_id\", how=\"left\")\n",
    "# train = train[~train.item_category_id.isin([8, 80])]\n",
    "# # Clip outliers and  remove items with a negative sales price or item_cnt_day\n",
    "# train = train.query(\"(item_price>0) & (item_cnt_day>0)\")\n",
    "# train.loc[:, \"item_price\"] = train.loc[:, \"item_price\"].clip(0, train[\"item_price\"].quantile(0.9999))\n",
    "# train.loc[:, \"item_cnt_day\"] = train.loc[:, \"item_cnt_day\"].clip(0, train[\"item_cnt_day\"].quantile(0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data with a item_price greater than 0\n",
    "train = train[train[\"item_price\"] > 0]\n",
    "# Extract data with a item_priceof less than 50,000\n",
    "train = train[train[\"item_price\"] < 50000]\n",
    "# Extract data with item_cnt_day greater than 0\n",
    "train = train[train[\"item_cnt_day\"] > 0]\n",
    "# Extract data with item_cnt_day less than 1,000\n",
    "train = train[train[\"item_cnt_day\"] < 1000]\n",
    "\n",
    "train.loc[train[\"shop_id\"] == 0, \"shop_id\"] = 57\n",
    "train.loc[train[\"shop_id\"] == 1, \"shop_id\"] = 58\n",
    "train.loc[train[\"shop_id\"] == 11, \"shop_id\"] = 10\n",
    "train.loc[train[\"shop_id\"] == 40, \"shop_id\"] = 39\n",
    "\n",
    "#  Modify shop_id in test data\n",
    "test.loc[test[\"shop_id\"] == 0, \"shop_id\"] = 57\n",
    "test.loc[test[\"shop_id\"] == 1, \"shop_id\"] = 58\n",
    "test.loc[test[\"shop_id\"] == 11, \"shop_id\"] = 10\n",
    "test.loc[test[\"shop_id\"] == 40, \"shop_id\"] = 39\n",
    "\n",
    "# Leaking to imporve performance\n",
    "unique_test_shop_id = test[\"shop_id\"].unique()\n",
    "train = train[train[\"shop_id\"].isin(unique_test_shop_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053208,
     "end_time": "2021-04-28T18:11:34.280403",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.227195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050656,
     "end_time": "2021-04-28T18:11:34.384271",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.333615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create a training matrix similar to the test items by aggregating the sales to the month level and creating items for every possible combination of shops and items featured in each individual month of the training data. Additionally, concatenate test to train data to enable creation of features for the test items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.065775,
     "end_time": "2021-04-28T18:11:34.500373",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.434598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_testlike_train(sales_train, test=None):\n",
    "    # Create a date_block_num / item_id / shop_id index using all combinations of item_id and shop_id occurring within each date_block\n",
    "    # Optionally concatenate the test items to the end\n",
    "    indexlist = []\n",
    "    for i in sales_train.date_block_num.unique():\n",
    "        x = itertools.product(\n",
    "            [i],\n",
    "            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n",
    "            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n",
    "        )\n",
    "        indexlist.append(np.array(list(x)))\n",
    "    df = pd.DataFrame(\n",
    "        data=np.concatenate(indexlist, axis=0),\n",
    "        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    # Add revenue column to sales_train\n",
    "    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n",
    "    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n",
    "    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n",
    "        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n",
    "        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n",
    "    )\n",
    "\n",
    "    # Merge the grouped data with the index\n",
    "    df = df.merge(\n",
    "        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    if test is not None:\n",
    "        test[\"date_block_num\"] = 34\n",
    "        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
    "        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
    "        test[\"item_id\"] = test.item_id.astype(np.int16)\n",
    "        test = test.drop(columns=\"ID\")\n",
    "\n",
    "        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n",
    "\n",
    "    # Fill empty item_cnt entries with 0\n",
    "    df.item_cnt_month = df.item_cnt_month.fillna(0)\n",
    "    df.item_revenue_month = df.item_revenue_month.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 23.143354,
     "end_time": "2021-04-28T18:11:57.693556",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.550202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = create_testlike_train(train, test)\n",
    "del(test)\n",
    "matrix = fu.reduce_mem_usage(matrix, silent=False)\n",
    "oldcols = matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051209,
     "end_time": "2021-04-28T18:11:57.796356",
     "exception": false,
     "start_time": "2021-04-28T18:11:57.745147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering  \n",
    "In this section predictor feature columns are generated and added to the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050933,
     "end_time": "2021-04-28T18:11:57.900260",
     "exception": false,
     "start_time": "2021-04-28T18:11:57.849327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Item name groups discovered by name matching with fuzzywuzzy\n",
    "\n",
    "Items in the items table are ordered alphabetically according to the item_name field, so that similar items are generally listed next to each other. This ordering can be used to help group related items together, which could perhaps help share information between items.  \n",
    "\n",
    "The following cell groups similar items together by using the sting matching algorithm fuzzywuzzy (https://github.com/seatgeek/fuzzywuzzy) is used to measure the similarity of each item to the item preceding it in the items table, then grouping adjacent items into groups unless the similarity score is below a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.079826,
     "end_time": "2021-04-28T18:11:58.030271",
     "exception": false,
     "start_time": "2021-04-28T18:11:57.950445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "items.query(\"item_id>3564\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 10.183104,
     "end_time": "2021-04-28T18:12:08.265598",
     "exception": false,
     "start_time": "2021-04-28T18:11:58.082494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def add_item_name_groups(matrix, train, items, sim_thresh, feature_name=\"item_name_group\"):\n",
    "    def partialmatchgroups(items, sim_thresh=sim_thresh):\n",
    "        def strip_brackets(string):\n",
    "            string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "            string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "            return string\n",
    "\n",
    "        items = items.copy()\n",
    "        items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "        items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "\n",
    "        def partialcompare(s):\n",
    "            return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "\n",
    "        items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "        # Assign groups\n",
    "        grp = 0\n",
    "        for i in range(items.shape[0]):\n",
    "            items.loc[i, \"partialmatchgroup\"] = grp\n",
    "            if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "                grp += 1\n",
    "        items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "        return items\n",
    "\n",
    "    items = partialmatchgroups(items)\n",
    "    items = items.rename(columns={\"partialmatchgroup\": feature_name})\n",
    "    items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n",
    "\n",
    "    items[feature_name] = items[feature_name].apply(str)\n",
    "    items[feature_name] = items[feature_name].factorize()[0]\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "    train = train.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "    return matrix, train\n",
    "\n",
    "\n",
    "matrix, train = add_item_name_groups(matrix, train, items, 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050709,
     "end_time": "2021-04-28T18:12:08.369960",
     "exception": false,
     "start_time": "2021-04-28T18:12:08.319251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Music artist extraction or first word of item name  \n",
    "\n",
    "The \"item_name\" field of items in the music categories typically begin with the artist's name marked with one of three patterns: either all uppercase, separated from the release title by a doublespace, or separated by dot-space (. ). Regex expressions are used to extract artist names from music categories using these rules. For non-music categories, the first word in the name is extracted instead.  \n",
    "\n",
    "This feature adds a more general grouping to the preceding item name group features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.989192,
     "end_time": "2021-04-28T18:12:11.411023",
     "exception": false,
     "start_time": "2021-04-28T18:12:08.421831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def add_first_word_features(matrix, items=items, feature_name=\"artist_name_or_first_word\"):\n",
    "    # This extracts artist names for music categories and adds them as a feature.\n",
    "    def extract_artist(st):\n",
    "        st = st.strip()\n",
    "        if st.startswith(\"V/A\"):\n",
    "            artist = \"V/A\"\n",
    "        elif st.startswith(\"СБ\"):\n",
    "            artist = \"СБ\"\n",
    "        else:\n",
    "            # Retrieves artist names using the double space or all uppercase pattern\n",
    "            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n",
    "            match_dubspace = mus_artist_dubspace.match(st)\n",
    "            mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n",
    "            match_capsonly = mus_artist_capsonly.match(st)\n",
    "            candidates = [match_dubspace, match_capsonly]\n",
    "            candidates = [m[0] for m in candidates if m is not None]\n",
    "            # Sometimes one of the patterns catches some extra words so choose the shortest one\n",
    "            if len(candidates):\n",
    "                artist = min(candidates, key=len)\n",
    "            else:\n",
    "                # If neither of the previous patterns found something, use the dot-space pattern\n",
    "                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n",
    "                match = mus_artist_dotspace.match(st)\n",
    "                if match:\n",
    "                    artist = match[0]\n",
    "                else:\n",
    "                    artist = \"\"\n",
    "        artist = artist.upper()\n",
    "        artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n",
    "        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n",
    "        artist = artist.strip()\n",
    "        return artist\n",
    "\n",
    "    items = items.copy()\n",
    "    all_stopwords = stopwords.words(\"russian\")\n",
    "    all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "    def first_word(string):\n",
    "        # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n",
    "        string = re.sub(r\"[^\\w\\s]\", \"\", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        tokens = string.lower().split()\n",
    "        tokens = [t for t in tokens if t not in all_stopwords]\n",
    "        token = tokens[0] if len(tokens) > 0 else \"\"\n",
    "        return token\n",
    "\n",
    "    music_categories = [55, 56, 57, 58, 59, 60]\n",
    "    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[\n",
    "        items.item_category_id.isin(music_categories), \"item_name\"\n",
    "    ].apply(extract_artist)\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other music\"\n",
    "    items.loc[~items.item_category_id.isin(music_categories), feature_name] = items.loc[\n",
    "        ~items.item_category_id.isin(music_categories), \"item_name\"\n",
    "    ].apply(first_word)\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other non-music\"\n",
    "    items[feature_name] = items[feature_name].factorize()[0]\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\",)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = add_first_word_features(\n",
    "    matrix, items=items, feature_name=\"artist_name_or_first_word\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051751,
     "end_time": "2021-04-28T18:12:11.513824",
     "exception": false,
     "start_time": "2021-04-28T18:12:11.462073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Item name length as a feature\n",
    "Surprisingly predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.574616,
     "end_time": "2021-04-28T18:12:13.140382",
     "exception": false,
     "start_time": "2021-04-28T18:12:11.565766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_item_name(string):\n",
    "    # Removes bracketed terms, special characters and extra whitespace\n",
    "    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "items[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\n",
    "items[\"item_name_length\"] = items[\"item_name\"].apply(len)\n",
    "matrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')\n",
    "items = items.drop(columns=['item_name_length', 'item_name_cleaned_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.963919,
     "end_time": "2021-04-28T18:12:15.155549",
     "exception": false,
     "start_time": "2021-04-28T18:12:13.191630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Created name features\")\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051242,
     "end_time": "2021-04-28T18:12:15.258377",
     "exception": false,
     "start_time": "2021-04-28T18:12:15.207135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Time features\n",
    "Time-dependent features\n",
    "* Item and shop age in months\n",
    "* Binary features for new items and shops (i.e. first month of appearance)\n",
    "* Item and shop age in days\n",
    "* Number of days in the current month\n",
    "* Average item count per day in the last month - useful for new items which may not have been available for the full calendar month\n",
    "* Days since last sale of an item in each shop and all shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.086102,
     "end_time": "2021-04-28T18:12:15.396020",
     "exception": false,
     "start_time": "2021-04-28T18:12:15.309918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_time_features(m, train, correct_item_cnt_day=False):\n",
    "    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n",
    "\n",
    "    def item_shop_age_months(m):\n",
    "        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(lambda x: x - x.min())\n",
    "        # Sales tend to plateau after 12 months\n",
    "        m[\"new_item\"] = m[\"item_age\"] == 0\n",
    "        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n",
    "        m[\"shop_age\"] = m.groupby(\"shop_id\")[\"date_block_num\"].transform(lambda x: x - x.min()).astype(\"int8\")\n",
    "        return m\n",
    "\n",
    "    # Add dummy values for the test month so that features are created correctly\n",
    "    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n",
    "    dummies = dummies.assign(date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,)\n",
    "    train = pd.concat([train, dummies])\n",
    "    del dummies\n",
    "\n",
    "    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n",
    "    month_last_day[~month_last_day.dt.is_month_end] = month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n",
    "    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n",
    "    month_first_day[~month_first_day.dt.is_month_start] = month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n",
    "    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n",
    "    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n",
    "    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n",
    "    first_shop_item_date = train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n",
    "    first_item_name_group_date = train.groupby(\"item_name_group\").date.min().rename(\"first_name_group_date\")\n",
    "\n",
    "    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\")\n",
    "    m = m.merge(first_item_name_group_date, left_on=\"item_name_group\", right_index=True, how=\"left\")\n",
    "\n",
    "    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n",
    "    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n",
    "    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n",
    "    m[\"item_in_shop_days\"] = m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n",
    "    m = m.drop(columns=\"item_first_sale_days\")\n",
    "    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] / m[\"item_in_shop_days\"]\n",
    "    m[\"month_length\"] = m[\"month_length\"].dt.days\n",
    "\n",
    "    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n",
    "    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n",
    "    m[\"first_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n",
    "    m[\"first_shop_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n",
    "    m[\"first_name_group_sale_days\"] = m[\"month_first_day\"] - m[\"first_name_group_date\"]\n",
    "    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n",
    "    m[\"first_item_sale_days\"] = m[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0,9999)\n",
    "    m[\"first_shop_item_sale_days\"] = m[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0,9999)\n",
    "    m[\"first_name_group_sale_days\"] = m[\"first_name_group_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0,9999)\n",
    "\n",
    "    # Add days since last sale\n",
    "    def last_sale_days(matrix):\n",
    "        last_shop_item_dates = []\n",
    "        for dbn in range(1, 35):\n",
    "            lsid_temp = train.query(f\"date_block_num<{dbn}\").groupby([\"shop_id\", \"item_id\"]).date.max().rename(\"last_shop_item_sale_date\").reset_index()\n",
    "            lsid_temp[\"date_block_num\"] = dbn\n",
    "            last_shop_item_dates.append(lsid_temp)\n",
    "                       \n",
    "        last_shop_item_dates = pd.concat(last_shop_item_dates)\n",
    "        matrix = matrix.merge(last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\")\n",
    "        \n",
    "        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n",
    "            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n",
    "            m.loc[m[feat_name] > 2000, feat_name] = missingval\n",
    "            m.loc[m[feat_name].isna(), feat_name] = missingval\n",
    "            return m\n",
    "\n",
    "        matrix = days_since_last_feat(matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999)\n",
    "               \n",
    "        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\"])\n",
    "        return matrix\n",
    "\n",
    "    m = last_sale_days(m)\n",
    "    # Month\n",
    "    m[\"month\"] = m[\"month_first_day\"].dt.month\n",
    "\n",
    "    m = m.drop(\n",
    "        columns=[\n",
    "            \"first_day\",\n",
    "            \"month_first_day\",\n",
    "            \"month_last_day\",\n",
    "            \"first_shop_date\",\n",
    "            \"first_item_date\",\n",
    "            \"first_name_group_date\",\n",
    "            \"item_in_shop_days\",\n",
    "            \"first_shop_item_date\",\n",
    "            \"month_length\"\n",
    "        ],\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "\n",
    "    m = item_shop_age_months(m)\n",
    "\n",
    "    if correct_item_cnt_day == True:\n",
    "        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n",
    "        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 44.568578,
     "end_time": "2021-04-28T18:13:00.017133",
     "exception": false,
     "start_time": "2021-04-28T18:12:15.448555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_time_features(matrix, train, False)\n",
    "print(\"Time features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052653,
     "end_time": "2021-04-28T18:13:00.123210",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.070557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Clip the monthly item counts to match the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.097808,
     "end_time": "2021-04-28T18:13:00.274490",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.176682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(lower=0, upper=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052584,
     "end_time": "2021-04-28T18:13:00.379709",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.327125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Price features  \n",
    "* Last mean price for a month in which the item was sold\n",
    "* Difference of the last price from the historical mean (calculated with an expanding window)\n",
    "* Difference of the last mean price from the first sale price in the data (proxy for the release price)\n",
    "* Difference of the last mean price from the mean for the category in the same month.  \n",
    "\n",
    "It can be assumed that item price is related to sales, both in absolute terms and relative to other items of the same category and the same item's earlier price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.067926,
     "end_time": "2021-04-28T18:13:00.501112",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.433186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_price_features(matrix, train):\n",
    "    # Get mean prices per month from train dataframe\n",
    "    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n",
    "    price_features = pd.DataFrame(price_features)\n",
    "    price_features = price_features.reset_index()\n",
    "    # Calculate normalized differenced from mean category price per month\n",
    "    price_features = price_features.merge(\n",
    "        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n",
    "    )\n",
    "    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n",
    "        [\"date_block_num\", \"item_category_id\"]\n",
    "    )[\"item_price\"].transform(lambda x: (x - x.mean()) / x.mean())\n",
    "    # Retain only the necessary features\n",
    "    price_features = price_features[\n",
    "        [\n",
    "            \"date_block_num\",\n",
    "            \"item_id\",\n",
    "            \"item_price\",\n",
    "            \"norm_diff_cat_price\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        \"item_price\",\n",
    "        \"norm_diff_cat_price\",\n",
    "    ]\n",
    "    newnames = [\"last_\" + f for f in features]\n",
    "    aggs = {f: \"last\" for f in features}\n",
    "    renames = {f: \"last_\" + f for f in features}\n",
    "    features = []\n",
    "    for dbn in range(1, 35):\n",
    "        f_temp = (\n",
    "            price_features.query(f\"date_block_num<{dbn}\")\n",
    "            .groupby(\"item_id\")\n",
    "            .agg(aggs)\n",
    "            .rename(columns=renames)\n",
    "        )\n",
    "        f_temp[\"date_block_num\"] = dbn\n",
    "        features.append(f_temp)\n",
    "    features = pd.concat(features).reset_index()\n",
    "    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.30382,
     "end_time": "2021-04-28T18:13:04.858570",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.554750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_price_features(matrix, train)\n",
    "del(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052081,
     "end_time": "2021-04-28T18:13:04.964343",
     "exception": false,
     "start_time": "2021-04-28T18:13:04.912262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053893,
     "end_time": "2021-04-28T18:13:05.071867",
     "exception": false,
     "start_time": "2021-04-28T18:13:05.017974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Categorical features provided with the data, e.g. item category, and custom categories extracted from category and shop names, e.g. \"video games\", \"music\", \"PS4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.567779,
     "end_time": "2021-04-28T18:13:08.693370",
     "exception": false,
     "start_time": "2021-04-28T18:13:05.125591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = matrix.merge(items.drop(columns='item_name'), on='item_id', how='left')\n",
    "matrix = matrix.merge(item_categories_extra, on='item_category_id', how='left')\n",
    "del(item_categories_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.054288,
     "end_time": "2021-04-28T18:13:08.800954",
     "exception": false,
     "start_time": "2021-04-28T18:13:08.746666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "City that the shop is located in\n",
    "(from https://www.kaggle.com/dlarionov/feature-engineering-xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.896395,
     "end_time": "2021-04-28T18:13:10.753735",
     "exception": false,
     "start_time": "2021-04-28T18:13:08.857340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_city_codes(matrix, shops):\n",
    "    shops.loc[\n",
    "        shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n",
    "    ] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n",
    "    shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
    "    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n",
    "    shop_labels = shops[[\"shop_id\", \"city_code\"]]\n",
    "    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n",
    "    return matrix\n",
    "\n",
    "matrix = add_city_codes(matrix, shops)\n",
    "del(shops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053194,
     "end_time": "2021-04-28T18:13:10.860623",
     "exception": false,
     "start_time": "2021-04-28T18:13:10.807429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Shop sales profile clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.29023,
     "end_time": "2021-04-28T18:13:11.205754",
     "exception": false,
     "start_time": "2021-04-28T18:13:10.915524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def cluster_feature(matrix, target_feature, clust_feature, level_feature, n_components=4, n_clusters=5, aggfunc=\"mean\", exclude=None):\n",
    "    start_month = 20\n",
    "    end_month = 32\n",
    "    pt = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\n",
    "    if exclude is not None:\n",
    "        pt = matrix[~matrix[clust_feature].isin(exclude)]\n",
    "    pt = pt.pivot_table(values=target_feature, columns=clust_feature, index=level_feature, fill_value=0, aggfunc=aggfunc)\n",
    "    pt = pt.transpose()\n",
    "    pca = PCA(n_components=10)\n",
    "    components = pca.fit_transform(pt)\n",
    "    components = pd.DataFrame(components)\n",
    "    # Plot PCA explained variance\n",
    "    sns.set_theme()\n",
    "    features = list(range(pca.n_components_))\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "#     ax.bar(features, pca.explained_variance_ratio_, color=\"black\")\n",
    "    sns.barplot(x=features, y=pca.explained_variance_ratio_, ax=ax)\n",
    "    plt.title(\"Variance by PCA components\")\n",
    "    plt.xlabel(\"component\")\n",
    "    plt.ylabel(\"explained variance\")\n",
    "    plt.xticks(features)\n",
    "\n",
    "    scorelist = []\n",
    "    nrange = range(2, 10)\n",
    "    for n in nrange:\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n)\n",
    "        labels = clusterer.fit_predict(components)\n",
    "        silscore = silhouette_score(pt, labels)\n",
    "        scorelist.append(silscore)\n",
    "    ax = fig.add_subplot(122)\n",
    "    sns.lineplot(x=nrange, y=scorelist, ax=ax)\n",
    "    plt.title(\"Clustering quality by number of clusters\")\n",
    "    plt.xlabel(\"n clusters\")\n",
    "    plt.ylabel(\"silhouette score\")\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(pt)\n",
    "    components = pd.DataFrame(components)\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n",
    "    labels = clusterer.fit_predict(components)\n",
    "    x = components[0]\n",
    "    y = components[1]\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.scatterplot(x=x, y=y, hue=labels, palette=sns.color_palette(\"hls\", n_clusters), ax=ax)\n",
    "    plt.title(\"Items by cluster\")\n",
    "    plt.xlabel(\"component 1 score\")\n",
    "    plt.ylabel(\"component 2 score\")\n",
    "    for i, txt in enumerate(pt.index.to_list()):\n",
    "        ax.annotate(str(txt), (x[i], y[i]))\n",
    "    groups = {}\n",
    "    for i, s in enumerate(pt.index):\n",
    "        groups[s] = labels[i]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.980666,
     "end_time": "2021-04-28T18:13:15.239410",
     "exception": false,
     "start_time": "2021-04-28T18:13:11.258744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_group_dict = cluster_feature(matrix, 'item_cnt_month', 'item_category_id', 'date_block_num', n_components=2, n_clusters=6, aggfunc=\"mean\", exclude =[])\n",
    "matrix['category_cluster'] = matrix['item_category_id'].map(category_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.477294,
     "end_time": "2021-04-28T18:13:18.773790",
     "exception": false,
     "start_time": "2021-04-28T18:13:15.296496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shop_group_dict = cluster_feature(matrix, 'item_cnt_month', 'shop_id', 'item_category_id', n_components=4, n_clusters=4, aggfunc=\"sum\", exclude=[36])\n",
    "shop_group_dict[36] = shop_group_dict[37]  # Shop36 added separately because it only has one month of data\n",
    "matrix['shop_cluster'] = matrix['shop_id'].map(shop_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9.033337,
     "end_time": "2021-04-28T18:13:27.865241",
     "exception": false,
     "start_time": "2021-04-28T18:13:18.831904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory\n",
    "matrix.to_pickle(\"matrixcheckpoint_1.pkl\")\n",
    "print(\"Saved matrixcheckpoint 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058948,
     "end_time": "2021-04-28T18:13:27.984695",
     "exception": false,
     "start_time": "2021-04-28T18:13:27.925747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Number of unique item features  \n",
    "i.e. counts of new and unique items in the current month  \n",
    "This counts the number of unique and new items sold in the current month and also category, which could have a positive relationship to sales. As this can also be calculated for the test set (assuming that the set of test items is the set of items that were sold in the test month) this is a kind of data leak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 55.648299,
     "end_time": "2021-04-28T18:14:23.691969",
     "exception": false,
     "start_time": "2021-04-28T18:13:28.043670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniques(matrix, groupers, name, limitation=None):\n",
    "    if limitation is not None:\n",
    "        s = matrix.query(limitation).groupby(groupers).item_id.nunique().rename(name).reset_index()\n",
    "    else:\n",
    "        s = matrix.groupby(groupers).item_id.nunique().rename(name).reset_index()\n",
    "    matrix = matrix.merge(s, on=groupers, how='left')\n",
    "    matrix[name] = matrix[name].fillna(0)\n",
    "    return matrix\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\"], \"unique_items_month\")\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_name_group\"], \"name_group_unique_month\")\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\", \"item_name_group\"], \"name_group_cat_unique_month\")\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_name_group\"], \"name_group_new_unique_month\", limitation=\"new_item==True\")\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\", \"item_name_group\"], \"name_group_new_cat_unique_month\", limitation=\"new_item==True\")\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"artist_name_or_first_word\"], \"first_word_unique_month\")\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"], \"first_word_cat_unique_month\")\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"artist_name_or_first_word\"], \"first_word_new_unique_month\", limitation=\"new_item==True\")\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"], \"first_word_new_cat_unique_month\", limitation=\"new_item==True\")\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\"], \"unique_items_cat\")\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\"], \"new_items_cat\", limitation=\"new_item==True\")\n",
    "matrix = uniques(matrix, [\"date_block_num\"], \"new_items_month\", limitation=\"new_item==True\")\n",
    "\n",
    "matrix[\"cat_items_proportion\"] = matrix[\"unique_items_cat\"] / matrix[\"unique_items_month\"]\n",
    "matrix[\"name_group_new_proportion_month\"] = matrix[\"name_group_new_unique_month\"] / matrix[\"name_group_unique_month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.661615,
     "end_time": "2021-04-28T18:14:25.413563",
     "exception": false,
     "start_time": "2021-04-28T18:14:23.751948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = matrix.drop(columns=['unique_items_month', 'name_group_unique_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.877312,
     "end_time": "2021-04-28T18:14:31.349756",
     "exception": false,
     "start_time": "2021-04-28T18:14:25.472444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058252,
     "end_time": "2021-04-28T18:14:31.467011",
     "exception": false,
     "start_time": "2021-04-28T18:14:31.408759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Percentage change in an aggregate feature over a specified period  \n",
    "e.g. change in total shop revenue compared to the previous month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.074182,
     "end_time": "2021-04-28T18:14:31.600743",
     "exception": false,
     "start_time": "2021-04-28T18:14:31.526561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_pct_change(matrix, group_feats, target=\"item_cnt_month\", aggfunc=\"mean\", periods=1, lag=1, clip_value=None):\n",
    "    periods = fu.list_if_not(periods, int)\n",
    "    group_feats = fu.list_if_not(group_feats)\n",
    "    group_feats_full = [\"date_block_num\"] + group_feats\n",
    "    dat = matrix.pivot_table(index=group_feats + [\"date_block_num\"], values=target, aggfunc=aggfunc, fill_value=0, dropna=False).astype(\"float32\")\n",
    "    for g in group_feats:\n",
    "        firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "        dat = dat.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "        dat.loc[dat.index.get_level_values(\"date_block_num\") < dat[\"firsts\"], target] = float(\"nan\")\n",
    "        del dat[\"firsts\"]\n",
    "    for period in periods:\n",
    "        feat_name = \"_\".join(group_feats + [target] + [aggfunc] + [\"delta\"] + [str(period)] + [f\"lag_{lag}\"])\n",
    "        print(f\"Adding feature {feat_name}\")\n",
    "        dat = dat.groupby(group_feats)[target].transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\")).rename(feat_name)\n",
    "        if clip_value is not None:\n",
    "            dat = dat.clip(lower=-clip_value, upper=clip_value)\n",
    "    dat = dat.reset_index()\n",
    "    dat[\"date_block_num\"] += lag\n",
    "    matrix = matrix.merge(dat, on=[\"date_block_num\"] + group_feats, how=\"left\")\n",
    "    matrix[feat_name] = fu.reduce_mem_usage(matrix[feat_name])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 55.239396,
     "end_time": "2021-04-28T18:15:26.899005",
     "exception": false,
     "start_time": "2021-04-28T18:14:31.659609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_name_group\"], \"item_cnt_month\", clip_value=3)\n",
    "# Delta 1 features lagged by 12 months, intended to capture seasonal trends\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\n",
    "gc.collect()\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.450712,
     "end_time": "2021-04-28T18:15:28.412578",
     "exception": false,
     "start_time": "2021-04-28T18:15:26.961866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix.to_pickle(\"matrixcheckpoint_2.pkl\")\n",
    "print(\"Saved matrixcheckpoint 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063575,
     "end_time": "2021-04-28T18:15:28.537486",
     "exception": false,
     "start_time": "2021-04-28T18:15:28.473911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Rolling mean features\n",
    "Sales from previous months are a good predictor of future sales, but chance fluctuations mean that an average of several previous months may be more reliable than counts from a single month. Pandas has several windowing functions for time series built in, 3 of which are demonstrated below : expanding (all previous timepoints), rolling (a fixed number of previous timepoints) and exponential (a weighted window with weights which decrease with time distance before the current point). These are compared below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.452093,
     "end_time": "2021-04-28T18:15:29.050610",
     "exception": false,
     "start_time": "2021-04-28T18:15:28.598517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shop_id = 16\n",
    "item_id = 482\n",
    "im = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\n",
    "im['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\n",
    "im['expanding mean'] = im['item_cnt_month'].expanding().mean()\n",
    "im['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\n",
    "im = im.set_index('date_block_num')\n",
    "ax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.088969,
     "end_time": "2021-04-28T18:15:29.204666",
     "exception": false,
     "start_time": "2021-04-28T18:15:29.115697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_rolling_stats(\n",
    "    matrix,\n",
    "    features,\n",
    "    window=12,\n",
    "    kind=\"rolling\",\n",
    "    argfeat=\"item_cnt_month\",\n",
    "    aggfunc=\"mean\",\n",
    "    rolling_aggfunc=\"mean\",\n",
    "    dtype=\"float16\",\n",
    "    reshape_source=True,\n",
    "    lag_offset=0,\n",
    "):\n",
    "    # General purpose windowed aggregation function\n",
    "    def rolling_stat(matrix, source, feats, feat_name, window=12, argfeat=\"item_cnt_month\", aggfunc=\"mean\", dtype=dtype, lag_offset=0):\n",
    "        # Calculate a statistic on a windowed section of a source table grouping on specific features\n",
    "        store = []\n",
    "        for i in range(2 + lag_offset, 35 + lag_offset):\n",
    "            if len(feats) > 0:\n",
    "                mes = (\n",
    "                    source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n",
    "                    .groupby(feats)[argfeat]\n",
    "                    .agg(aggfunc)\n",
    "                    .astype(dtype)\n",
    "                    .rename(feat_name)\n",
    "                    .reset_index()\n",
    "                )\n",
    "            else:\n",
    "                mes = {}\n",
    "                mes[feat_name] = source.loc[source.date_block_num.isin(range(max([i - window, 0]), i)), argfeat].agg(aggfunc).astype(dtype)\n",
    "                mes = pd.DataFrame(data=mes, index=[i])\n",
    "            mes[\"date_block_num\"] = i - lag_offset\n",
    "            store.append(mes)\n",
    "        store = pd.concat(store)\n",
    "        matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n",
    "        return matrix\n",
    "\n",
    "    # The reshaped source aggregates features at the month level, fills in missing months with zeros and sets months before the feature level's first appearance to nan\n",
    "    if (reshape_source == True) or (kind == \"ewm\"):\n",
    "        source = matrix.pivot_table(index=features + [\"date_block_num\"], values=argfeat, aggfunc=aggfunc, fill_value=0, dropna=False).astype(dtype)\n",
    "        for g in features:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            source.loc[source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat] = float(\"nan\")\n",
    "            del source[\"firsts\"]\n",
    "        source = source.reset_index()\n",
    "    else:\n",
    "        source = matrix\n",
    "\n",
    "    if kind == \"rolling\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(matrix, source, features, feat_name, window=window, argfeat=argfeat, aggfunc=rolling_aggfunc, dtype=dtype, lag_offset=lag_offset)\n",
    "    elif kind == \"expanding\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(matrix, source, features, feat_name, window=100, argfeat=argfeat, aggfunc=aggfunc, dtype=dtype, lag_offset=lag_offset)\n",
    "    elif kind == \"ewm\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        source[feat_name] = source.groupby(features)[argfeat].ewm(halflife=window, min_periods=1).agg(rolling_aggfunc).to_numpy(dtype=dtype)\n",
    "        del source[argfeat]\n",
    "        #         source = source.reset_index()\n",
    "        source[\"date_block_num\"] += 1 - lag_offset\n",
    "        return matrix.merge(source, on=[\"date_block_num\"] + features, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063375,
     "end_time": "2021-04-28T18:15:29.331871",
     "exception": false,
     "start_time": "2021-04-28T18:15:29.268496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Implement rolling mean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1905.447107,
     "end_time": "2021-04-28T18:47:14.842926",
     "exception": false,
     "start_time": "2021-04-28T18:15:29.395819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"category_cluster\"], window=12)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], window=12)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\", \"item_category_id\", \"new_item\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"supercategory_id\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"shop_cluster\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_cluster\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_cluster\", \"item_name_group\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"category_cluster\", \"item_age\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"category_cluster\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"item_id\"], window=12)\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"artist_name_or_first_word\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_category_id\", \"item_age\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_category_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"item_category_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_category_id\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_name_group\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_name_group\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(matrix, [\"item_name_group\"], window=12)\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], kind=\"ewm\", window=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same but with median as monthly aggregation function for robustness to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1905.447107,
     "end_time": "2021-04-28T18:47:14.842926",
     "exception": false,
     "start_time": "2021-04-28T18:15:29.395819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"category_cluster\"], window=12, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"new_item\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\"], window=12, aggfunc=\"median\")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], window=12, aggfunc=\"median\")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\", \"item_category_id\", \"new_item\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"supercategory_id\", \"new_item\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_cluster\", \"item_id\"], kind=\"ewm\", window=1, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_cluster\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_cluster\", \"item_name_group\", \"new_item\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"category_cluster\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"category_cluster\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"item_id\"], window=12, aggfunc=\"median\")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], window=12, aggfunc=\"median\")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"artist_name_or_first_word\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"artist_name_or_first_word\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_category_id\"], window=12, aggfunc=\"median\")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_category_id\"], kind=\"ewm\", window=1, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    "    aggfunc=\"median\",\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_name_group\", \"item_age\"], window=12, reshape_source=False, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_name_group\"], kind=\"ewm\", window=1, aggfunc=\"median\"\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_name_group\"], window=12, aggfunc=\"median\")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], window=12, aggfunc=\"median\")\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], kind=\"ewm\", window=1, aggfunc=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 17.789665,
     "end_time": "2021-04-28T18:47:32.707445",
     "exception": false,
     "start_time": "2021-04-28T18:47:14.917780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(\n",
    "    matrix, oldcols\n",
    ")  # Use this function periodically to downcast dtypes to save memory\n",
    "matrix.to_pickle(\"matrixcheckpoint_3.pkl\")\n",
    "print(\"Saved matrixcheckpoint 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.448793,
     "end_time": "2021-04-28T18:47:36.713076",
     "exception": false,
     "start_time": "2021-04-28T18:47:36.264283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following code block calculates windowed mean sales features with day resolution accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 92.446203,
     "end_time": "2021-04-28T18:49:09.252422",
     "exception": false,
     "start_time": "2021-04-28T18:47:36.806219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summed sales & accurate windowed mean sales per day features\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_id\"],\n",
    "    aggfunc=\"sum\",\n",
    "    rolling_aggfunc=\"sum\",\n",
    "    kind=\"rolling\",\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_id\"],\n",
    "    aggfunc=\"sum\",\n",
    "    rolling_aggfunc=\"sum\",\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix[\"1year\"] = 365\n",
    "matrix[\"item_id_day_mean_expanding\"] = matrix[\n",
    "    \"item_id_item_cnt_month_sum_expanding_sum\"\n",
    "] / matrix[[\"first_item_sale_days\"]].min(axis=1)\n",
    "matrix[\"shop_id_item_id_day_mean_win_12\"] = matrix[\n",
    "    \"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"\n",
    "] / matrix[[\"first_item_sale_days\", \"shop_open_days\", \"1year\"]].min(axis=1)\n",
    "matrix.loc[matrix.new_item == True, \"item_id_day_mean_expanding\",] = float(\"nan\")\n",
    "matrix = matrix.drop(columns=[\"1year\", \"item_id_item_cnt_month_sum_expanding_sum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.217115,
     "end_time": "2021-04-28T18:49:09.640666",
     "exception": false,
     "start_time": "2021-04-28T18:49:09.423551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Revenue features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 91.965703,
     "end_time": "2021-04-28T18:50:41.781069",
     "exception": false,
     "start_time": "2021-04-28T18:49:09.815366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\"],\n",
    "    window=12,\n",
    "    argfeat=\"item_revenue_month\",\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090066,
     "end_time": "2021-04-28T18:50:41.952101",
     "exception": false,
     "start_time": "2021-04-28T18:50:41.862035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Windowed unique item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 28.595028,
     "end_time": "2021-04-28T18:51:10.635694",
     "exception": false,
     "start_time": "2021-04-28T18:50:42.040666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_category_id\"],\n",
    "    argfeat=\"new_items_cat\",\n",
    "    window=12,\n",
    "    reshape_source=True,\n",
    "    lag_offset=1,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_name_group\"],\n",
    "    argfeat=\"name_group_new_unique_month\",\n",
    "    window=12,\n",
    "    reshape_source=True,\n",
    "    lag_offset=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.07835,
     "end_time": "2021-04-28T18:51:10.792048",
     "exception": false,
     "start_time": "2021-04-28T18:51:10.713698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ratio of new items in category with mean over the previous year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.12446,
     "end_time": "2021-04-28T18:51:10.993974",
     "exception": false,
     "start_time": "2021-04-28T18:51:10.869514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix[\"new_items_cat_1_12_ratio\"] = (\n",
    "    matrix[\"new_items_cat\"]\n",
    "    / matrix[\"item_category_id_new_items_cat_mean_rolling_mean_win_12\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 7.661105,
     "end_time": "2021-04-28T18:51:18.734463",
     "exception": false,
     "start_time": "2021-04-28T18:51:11.073358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n",
    "matrix.to_pickle(\"matrixcheckpoint_5.pkl\")\n",
    "print(\"Saved matrixcheckpoint 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.108926,
     "end_time": "2021-04-28T18:51:19.025566",
     "exception": false,
     "start_time": "2021-04-28T18:51:18.916640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lagged features  \n",
    "Values for the same shop-item combination from the previous month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.091696,
     "end_time": "2021-04-28T18:51:19.215776",
     "exception": false,
     "start_time": "2021-04-28T18:51:19.124080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_lag_feature(matrix, lag_feature, lags):\n",
    "    for lag in lags:\n",
    "        newname = lag_feature + f\"_lag_{lag}\"\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n",
    "        targetseries[\"date_block_num\"] += lag\n",
    "        targetseries = targetseries.rename(columns={lag_feature: newname})\n",
    "        matrix = matrix.merge(\n",
    "            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n",
    "        )\n",
    "        matrix.loc[\n",
    "            (matrix.item_age >= lag) & (matrix.shop_age >= lag) & (matrix[newname].isna()),\n",
    "            newname,\n",
    "        ] = 0\n",
    "    #         matrix[newname] = fu.reduce_mem_usage(matrix[newname])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 120.906823,
     "end_time": "2021-04-28T18:53:20.206272",
     "exception": false,
     "start_time": "2021-04-28T18:51:19.299449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1,2,3])\n",
    "matrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1, 2, 3])\n",
    "matrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\n",
    "gc.collect()\n",
    "print(\"Lag features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084107,
     "end_time": "2021-04-28T18:53:20.373538",
     "exception": false,
     "start_time": "2021-04-28T18:53:20.289431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mean encodings\n",
    "The mean value of a target feature for each level of a categorical feature or combination of categorical features, lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.093518,
     "end_time": "2021-04-28T18:53:20.548179",
     "exception": false,
     "start_time": "2021-04-28T18:53:20.454661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_apply_ME(\n",
    "    matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\", aggfunc=\"mean\"\n",
    "):\n",
    "    grouping_fields = fu.list_if_not(grouping_fields)\n",
    "    for lag in lags:\n",
    "        newname = \"_\".join(grouping_fields + [target] + [aggfunc] + [f\"lag_{lag}\"])\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        me_series = (\n",
    "            matrix.groupby([\"date_block_num\"] + grouping_fields)[target]\n",
    "            .agg(aggfunc)\n",
    "            .rename(newname)\n",
    "            .reset_index()\n",
    "        )\n",
    "        me_series[\"date_block_num\"] += lag\n",
    "        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n",
    "        del me_series\n",
    "        matrix[newname] = matrix[newname].fillna(0)\n",
    "        for g in grouping_fields:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            matrix = matrix.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            matrix.loc[\n",
    "                matrix[\"date_block_num\"] < (matrix[\"firsts\"] + (lag)), newname\n",
    "            ] = float(\"nan\")\n",
    "            del matrix[\"firsts\"]\n",
    "        matrix[newname] = fu.reduce_mem_usage(matrix[newname])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 116.204762,
     "end_time": "2021-04-28T18:55:16.832993",
     "exception": false,
     "start_time": "2021-04-28T18:53:20.628231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\", aggfunc=\"sum\")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"], [1], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"], [1])\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"supercategory_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(\n",
    "    matrix, [\"shop_id\", \"item_category_id\"], [1], target=\"item_cnt_month\"\n",
    ")\n",
    "matrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 116.204762,
     "end_time": "2021-04-28T18:55:16.832993",
     "exception": false,
     "start_time": "2021-04-28T18:53:20.628231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = create_apply_ME(\n",
    "    matrix, [\"item_name_group\"], target=\"item_cnt_month\", aggfunc=\"median\"\n",
    ")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"], [1], target=\"item_cnt_month\", aggfunc=\"median\")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"], [1], aggfunc=\"median\")\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"], aggfunc=\"median\")\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"], aggfunc=\"median\")\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"], target=\"item_cnt_month\", aggfunc=\"median\")\n",
    "matrix = create_apply_ME(matrix, [\"supercategory_id\"], aggfunc=\"median\")\n",
    "matrix = create_apply_ME(\n",
    "    matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\", aggfunc=\"median\"\n",
    ")\n",
    "matrix = create_apply_ME(\n",
    "    matrix, [\"shop_id\", \"item_category_id\"], [1], target=\"item_cnt_month\", aggfunc=\"median\"\n",
    ")\n",
    "matrix = create_apply_ME(\n",
    "    matrix, [\"shop_cluster\", \"item_id\"], target=\"item_cnt_month\", aggfunc=\"median\"\n",
    ")\n",
    "matrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"], aggfunc=\"median\")\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"], aggfunc=\"median\")\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"], aggfunc=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.085228,
     "end_time": "2021-04-28T18:55:17.004209",
     "exception": false,
     "start_time": "2021-04-28T18:55:16.918981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ratios between recent sales and rolling 12 month means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.12325,
     "end_time": "2021-04-28T18:55:17.213537",
     "exception": false,
     "start_time": "2021-04-28T18:55:17.090287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_id_item_cnt_month_mean_lag_1\"]\n",
    "    / matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n",
    ")\n",
    "matrix[\"shop_id_item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_cnt_day_avg_lag_1\"] / matrix[\"shop_id_item_id_day_mean_win_12\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 11.498338,
     "end_time": "2021-04-28T18:55:28.796406",
     "exception": false,
     "start_time": "2021-04-28T18:55:17.298068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix, oldcols = fu.shrink_mem_new_cols(matrix, oldcols)\n",
    "matrix.to_pickle(\"matrixcheckpoint_6.pkl\")\n",
    "print(\"Saved matrixcheckpoint 6\")\n",
    "gc.collect()\n",
    "print(\"Mean encoding features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"matrixcheckpoint_6.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.295401,
     "end_time": "2021-04-28T18:55:46.830211",
     "exception": false,
     "start_time": "2021-04-28T18:55:44.534810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some columns that were used to generate other features can now be discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.783446,
     "end_time": "2021-04-28T18:55:54.944188",
     "exception": false,
     "start_time": "2021-04-28T18:55:49.160742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "surplus_columns = [\n",
    "    \"item_revenue_month\",\n",
    "    \"item_cnt_day_avg\",\n",
    "    \"item_name_group\",\n",
    "    \"artist_name_or_first_word\",\n",
    "    \"item_age\",\n",
    "    \"shop_open_days\",\n",
    "    \"shop_age\",\n",
    "    \"platform_id\",\n",
    "    \"supercategory_id\",\n",
    "    \"city_code\",\n",
    "    \"category_cluster\",\n",
    "    \"shop_cluster\",\n",
    "    \"new_items_cat\",\n",
    "#     \"shop_id_item_id_day_mean_win_12\",\n",
    "#     \"item_id_item_cnt_month_mean_rolling_mean_win_12\",\n",
    "]\n",
    "matrix = matrix.drop(columns=surplus_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084959,
     "end_time": "2021-04-28T18:55:55.115949",
     "exception": false,
     "start_time": "2021-04-28T18:55:55.030990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predictive words in item_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.174513,
     "end_time": "2021-04-28T18:55:55.375271",
     "exception": false,
     "start_time": "2021-04-28T18:55:55.200758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.117744,
     "end_time": "2021-04-28T18:55:55.579227",
     "exception": false,
     "start_time": "2021-04-28T18:55:55.461483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "def name_token_feats(matrix, items, k=50, item_n_threshold=5, target_month_start=33):\n",
    "    def name_correction(st):\n",
    "        st = re.sub(r\"[^\\w\\s]\", \"\", st)\n",
    "        st = re.sub(r\"\\s{2,}\", \" \", st)\n",
    "        st = st.lower().strip()\n",
    "        return st\n",
    "\n",
    "    items[\"item_name_clean\"] = items[\"item_name\"].apply(name_correction)\n",
    "\n",
    "    def create_item_id_bow_matrix(items):\n",
    "        all_stopwords = stopwords.words(\"russian\")\n",
    "        all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "        vectorizer = CountVectorizer(stop_words=all_stopwords)\n",
    "        X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n",
    "        X = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "        print(f\"{len(vectorizer.vocabulary_)} words found in all items\")\n",
    "        featuremap = {\n",
    "            col: \"word_\" + token\n",
    "            for col, token in zip(\n",
    "                range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names()\n",
    "            )\n",
    "        }\n",
    "        X = X.rename(columns=featuremap)\n",
    "        return X\n",
    "\n",
    "    items_bow = create_item_id_bow_matrix(items)\n",
    "    items_bow = items_bow.clip(0, 1)\n",
    "    common_word_mask = items_bow.sum(axis=0) > item_n_threshold\n",
    "    target_items = matrix.query(f\"date_block_num>={target_month_start} & new_item==True\").item_id.unique()\n",
    "    target_item_mask = items_bow.loc[target_items, :].sum(axis=0) > 1\n",
    "    items_bow = items_bow.loc[:, common_word_mask & target_item_mask]\n",
    "    print(f\"{items_bow.shape[1]} words of interest\")\n",
    "    mxbow = matrix[[\"date_block_num\", \"item_id\", \"item_cnt_month\"]].query(\"date_block_num<34\")\n",
    "    mxbow = mxbow.merge(items_bow, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "    X = mxbow.drop(columns=[\"date_block_num\", \"item_id\", \"item_cnt_month\"])\n",
    "    y = mxbow[\"item_cnt_month\"].clip(0, 20)\n",
    "    selektor = SelectKBest(f_regression, k=k)\n",
    "    selektor.fit(X, y)\n",
    "    tokencols = X.columns[selektor.get_support()]\n",
    "    print(f\"{k} word features selected\")\n",
    "    return items_bow[tokencols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1213.623185,
     "end_time": "2021-04-28T19:16:09.288159",
     "exception": false,
     "start_time": "2021-04-28T18:55:55.664974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_frame = name_token_feats(matrix, items, k=50)\n",
    "matrix = matrix.merge(word_frame, left_on='item_id', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.089387,
     "end_time": "2021-04-28T19:16:09.466054",
     "exception": false,
     "start_time": "2021-04-28T19:16:09.376667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recursive feature elimination (RFE) perfomed on the feature matrix found the following word columns to be redundant - here they are dropped.  \n",
    "RFE is not performed in the notebook due to memory contraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.347565,
     "end_time": "2021-04-28T19:16:14.282026",
     "exception": false,
     "start_time": "2021-04-28T19:16:13.934461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sparsecols = [c for c in matrix.columns if pd.api.types.is_sparse(matrix[c].dtype)]\n",
    "matrix[sparsecols] = matrix[sparsecols].sparse.to_dense().astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 8.028085,
     "end_time": "2021-04-28T19:16:22.397834",
     "exception": false,
     "start_time": "2021-04-28T19:16:14.369749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix.to_pickle(\"checkpoint_test.pkl\")\n",
    "print(\"All features generated, dataframe saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.610262,
     "end_time": "2021-04-28T19:16:23.204949",
     "exception": false,
     "start_time": "2021-04-28T19:16:22.594687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # clean up old backups\n",
    "# import os\n",
    "# dirlist = os.listdir()\n",
    "# for f in dirlist:\n",
    "#     if f[:16] == 'matrixcheckpoint':\n",
    "#         os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.44176,
     "end_time": "2021-04-28T19:16:23.862813",
     "exception": false,
     "start_time": "2021-04-28T19:16:23.421053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08846,
     "end_time": "2021-04-28T19:16:24.040872",
     "exception": false,
     "start_time": "2021-04-28T19:16:23.952412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model fitting section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.098249,
     "end_time": "2021-04-28T19:16:24.227277",
     "exception": false,
     "start_time": "2021-04-28T19:16:24.129028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import futuresalesutility as fu\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.705662,
     "end_time": "2021-04-28T19:16:29.022815",
     "exception": false,
     "start_time": "2021-04-28T19:16:24.317153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"checkpoint_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 20.293087,
     "end_time": "2021-04-28T19:16:49.414172",
     "exception": false,
     "start_time": "2021-04-28T19:16:29.121085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Split train, validation, test sets from feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.258176,
     "end_time": "2021-04-28T19:16:49.761566",
     "exception": false,
     "start_time": "2021-04-28T19:16:49.503390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "def fit_booster(X_train, y_train, X_test=None, y_test=None, params=None, test_run = False, categoricals=[], dropcols=[], early_stopping=True):\n",
    "    # Regular booster fitting function\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"subsample_for_bin\": 300000,\n",
    "            \"n_estimators\": 5000\n",
    "        }\n",
    "\n",
    "    early_stopping_rounds=None\n",
    "\n",
    "    if early_stopping==True:\n",
    "        early_stopping_rounds=30\n",
    "       \n",
    "    if test_run:\n",
    "        eval_set=[(X_train, y_train)]\n",
    "    else:\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)]\n",
    "\n",
    "    booster = lgbm.LGBMRegressor(**params)\n",
    "    \n",
    "    categoricals = [c for c in categoricals if c in X_train.columns]\n",
    "\n",
    "    booster.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=eval_set,\n",
    "        eval_metric=[\"rmse\"],\n",
    "        verbose=10,\n",
    "        categorical_feature=categoricals,\n",
    "        early_stopping_rounds=early_stopping_rounds\n",
    "    )\n",
    "\n",
    "    if test_run:\n",
    "        X_test['item_cnt_month'] = booster.predict(X_test)\n",
    "        return booster, X_test\n",
    "    else:\n",
    "        return booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9.884487,
     "end_time": "2021-04-28T19:16:59.734499",
     "exception": false,
     "start_time": "2021-04-28T19:16:49.850012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keep_from_month=2\n",
    "test_month = 33\n",
    "gc.collect()\n",
    "dropcols = [\"shop_id\", \"digital\", \"item_id\", \"new_item\"]\n",
    "\n",
    "valid = matrix.drop(columns=dropcols).loc[matrix.date_block_num==test_month, :]\n",
    "train = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\n",
    "train = train[train.date_block_num >= keep_from_month]\n",
    "X_train = train.drop(columns=\"item_cnt_month\")\n",
    "y_train = train.item_cnt_month\n",
    "X_valid = valid.drop(columns=\"item_cnt_month\")\n",
    "y_valid = valid.item_cnt_month\n",
    "del(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_leaves\": 966,\n",
    "    \"cat_smooth\": 45.01680827234465,\n",
    "    \"min_child_samples\": 27,\n",
    "    \"min_child_weight\": 0.021144950289224463,\n",
    "    \"max_bin\": 214,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample_for_bin\": 300000,\n",
    "    \"min_data_in_bin\": 7,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"subsample\": 0.6,\n",
    "    \"subsample_freq\": 5,\n",
    "    \"n_estimators\": 2000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1221.866881,
     "end_time": "2021-04-28T19:37:21.690596",
     "exception": false,
     "start_time": "2021-04-28T19:16:59.823715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categoricals = [\n",
    "    'item_category_id',\n",
    "     'month',\n",
    "]\n",
    "\n",
    "booster = fit_booster(\n",
    "    X_train, y_train, X_valid, y_valid, params=params, test_run = False, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best iteration with default params and LR=0.01  \n",
    "1090]\ttraining's rmse: 0.754182\ttraining's l2: 0.56879\tvalid_1's rmse: 0.722489\tvalid_1's l2: 0.52199  \n",
    "Best iteration with optuna best params found on clipped dataset with LR=0.01  \n",
    "620]\ttraining's rmse: 0.61801\ttraining's l2: 0.381937\tvalid_1's rmse: 0.710575\tvalid_1's l2: 0.504917  \n",
    "Best iteration with optuna best params found on clipped dataset with LR=0.01 and item_id feature dropped  \n",
    "619]\ttraining's rmse: 0.619193\ttraining's l2: 0.3834\tvalid_1's rmse: 0.709474\tvalid_1's l2: 0.503353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.892104,
     "end_time": "2021-04-28T19:37:25.679539",
     "exception": false,
     "start_time": "2021-04-28T19:37:21.787435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = lgbm.plot_importance(booster, figsize=(10,50), height=0.5, importance_type=\"gain\", ignore_zero=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.111863,
     "end_time": "2021-04-28T19:37:25.904072",
     "exception": false,
     "start_time": "2021-04-28T19:37:25.792209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create the test submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 50.515225,
     "end_time": "2021-04-28T19:38:16.529825",
     "exception": false,
     "start_time": "2021-04-28T19:37:26.014600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\n",
    "\n",
    "keep_from_month = 2\n",
    "test_month = 34\n",
    "gc.collect()\n",
    "\n",
    "test = matrix.loc[matrix.date_block_num==test_month, :]\n",
    "# train = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\n",
    "# train = train[train.date_block_num >= keep_from_month]\n",
    "# X_train = train.drop(columns=\"item_cnt_month\")\n",
    "# y_train = train.item_cnt_month\n",
    "X_test = test.drop(columns=\"item_cnt_month\")\n",
    "y_test = test.item_cnt_month\n",
    "del(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 10.814403,
     "end_time": "2021-04-28T19:38:27.461147",
     "exception": false,
     "start_time": "2021-04-28T19:38:16.646744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assume that data to be predicted from is in X_test\n",
    "X_test['item_cnt_month'] = booster.predict(X_test.drop(columns=dropcols)).clip(0,20)\n",
    "X_test['shop_id'] = X_test['shop_id'].replace({11:10})\n",
    "\n",
    "# Merge the predictions with the provided template\n",
    "test_orig = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\n",
    "test = test_orig.merge(X_test[['shop_id','item_id','item_cnt_month']], on=['shop_id','item_id'], how='inner', copy=True)\n",
    "\n",
    "# test = test.rename(columns={'prediction':'item_cnt_month'})\n",
    "# Check that the indices math the original\n",
    "assert test_orig.equals(test[['ID','shop_id','item_id']])\n",
    "test[['ID','item_cnt_month']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.122639,
     "end_time": "2021-04-28T19:38:27.697031",
     "exception": false,
     "start_time": "2021-04-28T19:38:27.574392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"finished everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Mean predicted sales of digital items in non-digital shops is {X_test[(X_test.shop_id!=55) & (X_test.digital==1)].item_cnt_month.mean()}\")\n",
    "# print(f\"Mean predicted sales of non-digital items in digital shop 55 is {X_test[(X_test.shop_id==55) & (X_test.digital==0)].item_cnt_month.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5228.435769,
   "end_time": "2021-04-28T19:38:29.248622",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-28T18:11:20.812853",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
